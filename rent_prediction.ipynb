{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rent_prediction",
      "provenance": [],
      "authorship_tag": "ABX9TyPQxMOvJliLY6Fsa5W1RJbC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daniel8691/rental-housing-toronto/blob/master/rent_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfziV3gt5nqd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "261kpVNQ6oLn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = \"https://dataviz-class-1-dt.s3.us-east-2.amazonaws.com/transposed_complete_df.csv\"\n",
        "rental_df = pd.read_csv(url)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8KBVNXQMrw5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "outputId": "6bf6b9ea-7cc5-4cf5-d985-cf75c9465046"
      },
      "source": [
        "rental_df.head()\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>postal_code</th>\n",
              "      <th>cluster_labels</th>\n",
              "      <th>geocode_lat</th>\n",
              "      <th>geocode_lng</th>\n",
              "      <th>address</th>\n",
              "      <th>num_sqft</th>\n",
              "      <th>rent</th>\n",
              "      <th>num_bathrooms</th>\n",
              "      <th>price_per_sqft</th>\n",
              "      <th>metro_station</th>\n",
              "      <th>train_station</th>\n",
              "      <th>bus_station</th>\n",
              "      <th>bus_stop</th>\n",
              "      <th>shopping_mall</th>\n",
              "      <th>shopping_plaza</th>\n",
              "      <th>grocery_store</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>M1H</td>\n",
              "      <td>2</td>\n",
              "      <td>43.781624</td>\n",
              "      <td>-79.247376</td>\n",
              "      <td>36 Lee Centre Drive, Toronto</td>\n",
              "      <td>750.0</td>\n",
              "      <td>1925.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.566667</td>\n",
              "      <td>5609</td>\n",
              "      <td>4931</td>\n",
              "      <td>3924</td>\n",
              "      <td>2912</td>\n",
              "      <td>17655</td>\n",
              "      <td>8666</td>\n",
              "      <td>3345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>M1H</td>\n",
              "      <td>2</td>\n",
              "      <td>43.781660</td>\n",
              "      <td>-79.247547</td>\n",
              "      <td>38 Lee Centre Drive, Toronto</td>\n",
              "      <td>650.0</td>\n",
              "      <td>1950.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>5610</td>\n",
              "      <td>4944</td>\n",
              "      <td>3921</td>\n",
              "      <td>2916</td>\n",
              "      <td>17644</td>\n",
              "      <td>8651</td>\n",
              "      <td>3335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>M1H</td>\n",
              "      <td>2</td>\n",
              "      <td>43.781661</td>\n",
              "      <td>-79.245271</td>\n",
              "      <td>11 Lee Centre Drive, Toronto</td>\n",
              "      <td>650.0</td>\n",
              "      <td>1950.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>5655</td>\n",
              "      <td>4799</td>\n",
              "      <td>4011</td>\n",
              "      <td>2915</td>\n",
              "      <td>17815</td>\n",
              "      <td>8816</td>\n",
              "      <td>3493</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>M1H</td>\n",
              "      <td>2</td>\n",
              "      <td>43.780832</td>\n",
              "      <td>-79.247346</td>\n",
              "      <td>8 Lee Centre Drive, Toronto</td>\n",
              "      <td>650.0</td>\n",
              "      <td>1850.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.846154</td>\n",
              "      <td>5524</td>\n",
              "      <td>4876</td>\n",
              "      <td>3848</td>\n",
              "      <td>2823</td>\n",
              "      <td>17627</td>\n",
              "      <td>8707</td>\n",
              "      <td>3303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>M1H</td>\n",
              "      <td>2</td>\n",
              "      <td>43.773869</td>\n",
              "      <td>-79.249988</td>\n",
              "      <td>88 Grangeway Avenue, Toronto</td>\n",
              "      <td>650.0</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.076923</td>\n",
              "      <td>4721</td>\n",
              "      <td>4652</td>\n",
              "      <td>3074</td>\n",
              "      <td>2070</td>\n",
              "      <td>17170</td>\n",
              "      <td>8896</td>\n",
              "      <td>2796</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  postal_code  cluster_labels  ...  shopping_plaza  grocery_store\n",
              "0         M1H               2  ...            8666           3345\n",
              "1         M1H               2  ...            8651           3335\n",
              "2         M1H               2  ...            8816           3493\n",
              "3         M1H               2  ...            8707           3303\n",
              "4         M1H               2  ...            8896           2796\n",
              "\n",
              "[5 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNMzys6qNK0A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b76926b4-c63d-4598-e4aa-7b88402c4aa7"
      },
      "source": [
        "# find out the bathroom numbers for units \n",
        "rental_df['num_bathrooms'].unique()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1. , 0.5, 2. , 1.5, 0. , 3. ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8mludg8eQ1w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "be0116b4-9d08-4db3-a036-480f635af2b4"
      },
      "source": [
        "\n",
        "rental_df.columns.values"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['postal_code', 'cluster_labels', 'geocode_lat', 'geocode_lng',\n",
              "       'address', 'num_sqft', 'rent', 'num_bathrooms', 'price_per_sqft',\n",
              "       'metro_station', 'train_station', 'bus_station', 'bus_stop',\n",
              "       'shopping_mall', 'shopping_plaza', 'grocery_store'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a26u5_oeM5PW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "f135424c-0aa7-45d8-fcba-f41b36f96d21"
      },
      "source": [
        "# move rent to the very first column\n",
        "rental_df = rental_df[['rent','num_sqft', 'num_bathrooms',\n",
        "       'metro_station', 'train_station', 'bus_station', 'bus_stop',\n",
        "       'shopping_mall', 'shopping_plaza', 'grocery_store']]\n",
        "rental_df.head()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rent</th>\n",
              "      <th>num_sqft</th>\n",
              "      <th>num_bathrooms</th>\n",
              "      <th>metro_station</th>\n",
              "      <th>train_station</th>\n",
              "      <th>bus_station</th>\n",
              "      <th>bus_stop</th>\n",
              "      <th>shopping_mall</th>\n",
              "      <th>shopping_plaza</th>\n",
              "      <th>grocery_store</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1925.0</td>\n",
              "      <td>750.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5609</td>\n",
              "      <td>4931</td>\n",
              "      <td>3924</td>\n",
              "      <td>2912</td>\n",
              "      <td>17655</td>\n",
              "      <td>8666</td>\n",
              "      <td>3345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1950.0</td>\n",
              "      <td>650.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5610</td>\n",
              "      <td>4944</td>\n",
              "      <td>3921</td>\n",
              "      <td>2916</td>\n",
              "      <td>17644</td>\n",
              "      <td>8651</td>\n",
              "      <td>3335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1950.0</td>\n",
              "      <td>650.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5655</td>\n",
              "      <td>4799</td>\n",
              "      <td>4011</td>\n",
              "      <td>2915</td>\n",
              "      <td>17815</td>\n",
              "      <td>8816</td>\n",
              "      <td>3493</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1850.0</td>\n",
              "      <td>650.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5524</td>\n",
              "      <td>4876</td>\n",
              "      <td>3848</td>\n",
              "      <td>2823</td>\n",
              "      <td>17627</td>\n",
              "      <td>8707</td>\n",
              "      <td>3303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2000.0</td>\n",
              "      <td>650.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4721</td>\n",
              "      <td>4652</td>\n",
              "      <td>3074</td>\n",
              "      <td>2070</td>\n",
              "      <td>17170</td>\n",
              "      <td>8896</td>\n",
              "      <td>2796</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     rent  num_sqft  ...  shopping_plaza  grocery_store\n",
              "0  1925.0     750.0  ...            8666           3345\n",
              "1  1950.0     650.0  ...            8651           3335\n",
              "2  1950.0     650.0  ...            8816           3493\n",
              "3  1850.0     650.0  ...            8707           3303\n",
              "4  2000.0     650.0  ...            8896           2796\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7OLlLBNMdPM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "5a9010c3-2ee9-4a19-a0eb-2f435310518e"
      },
      "source": [
        "predictors = rental_df.iloc[:,1:]\n",
        "predictors.head()\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>num_sqft</th>\n",
              "      <th>num_bathrooms</th>\n",
              "      <th>metro_station</th>\n",
              "      <th>train_station</th>\n",
              "      <th>bus_station</th>\n",
              "      <th>bus_stop</th>\n",
              "      <th>shopping_mall</th>\n",
              "      <th>shopping_plaza</th>\n",
              "      <th>grocery_store</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>750.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5609</td>\n",
              "      <td>4931</td>\n",
              "      <td>3924</td>\n",
              "      <td>2912</td>\n",
              "      <td>17655</td>\n",
              "      <td>8666</td>\n",
              "      <td>3345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>650.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5610</td>\n",
              "      <td>4944</td>\n",
              "      <td>3921</td>\n",
              "      <td>2916</td>\n",
              "      <td>17644</td>\n",
              "      <td>8651</td>\n",
              "      <td>3335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>650.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5655</td>\n",
              "      <td>4799</td>\n",
              "      <td>4011</td>\n",
              "      <td>2915</td>\n",
              "      <td>17815</td>\n",
              "      <td>8816</td>\n",
              "      <td>3493</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>650.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5524</td>\n",
              "      <td>4876</td>\n",
              "      <td>3848</td>\n",
              "      <td>2823</td>\n",
              "      <td>17627</td>\n",
              "      <td>8707</td>\n",
              "      <td>3303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>650.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4721</td>\n",
              "      <td>4652</td>\n",
              "      <td>3074</td>\n",
              "      <td>2070</td>\n",
              "      <td>17170</td>\n",
              "      <td>8896</td>\n",
              "      <td>2796</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   num_sqft  num_bathrooms  ...  shopping_plaza  grocery_store\n",
              "0     750.0            1.0  ...            8666           3345\n",
              "1     650.0            1.0  ...            8651           3335\n",
              "2     650.0            1.0  ...            8816           3493\n",
              "3     650.0            1.0  ...            8707           3303\n",
              "4     650.0            1.0  ...            8896           2796\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMkchWWJfAmE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = predictors.values\n",
        "y = rental_df['rent'].values"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5W-o6X3ZfTgy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OYiuqJXfXYz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 42)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkP_f0sRfjMD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muuCL8mFfloq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaler = MinMaxScaler()"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-gY5H1IfrPq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = scaler.fit_transform(X_train)\n",
        "# don't fit the test set because we don't want to assume prior information about the test set\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWvzJwVXffYc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sequential model\n",
        "from tensorflow.keras.models import Sequential\n",
        "# Dense layers\n",
        "from tensorflow.keras.layers import Dense"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tK1BWon5so-R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c25fed9b-832a-406b-c0ef-7f1a2e43c3ad"
      },
      "source": [
        "# check that the dataset has 80% training and 20% testing data\n",
        "X_train.shape[0] / X.shape[0]"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGT72hFntp59",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "75b8ccd9-2680-46bd-b680-b94b3832d327"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(668, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YI8mYITAMb3Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(9, activation='relu'))\n",
        "model.add(Dense(9, activation='relu'))\n",
        "# model.add(Dense(9, activation='relu'))\n",
        "# model.add(Dense(9, activation='relu'))\n",
        "\n",
        "# one more layer with just one neuron since this is outputting the predicted price\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.compile(optimizer = 'adam', loss = 'mae')\n"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2jZHyoLt8k6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "77923cc6-daa8-47c4-b0b8-0b3a9a668e45"
      },
      "source": [
        "# train the model\n",
        "# check with test set as we train using the validation_data parameter\n",
        "model.fit(x=X_train, y=y_train, \n",
        "          validation_data = (X_test, y_test),\n",
        "          # set batch size so we don't pass in the entire training set at once (prevent overfitting)\n",
        "          # focus on smaller batch\n",
        "          batch_size = 128,\n",
        "          # run 400 times\n",
        "          epochs = 900)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/900\n",
            "6/6 [==============================] - 0s 22ms/step - loss: 2123.3208 - val_loss: 2128.4683\n",
            "Epoch 2/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2123.2375 - val_loss: 2128.3833\n",
            "Epoch 3/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2123.1484 - val_loss: 2128.2927\n",
            "Epoch 4/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2123.0522 - val_loss: 2128.1938\n",
            "Epoch 5/900\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 2122.9482 - val_loss: 2128.0879\n",
            "Epoch 6/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2122.8376 - val_loss: 2127.9741\n",
            "Epoch 7/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2122.7188 - val_loss: 2127.8530\n",
            "Epoch 8/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2122.5923 - val_loss: 2127.7231\n",
            "Epoch 9/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2122.4573 - val_loss: 2127.5842\n",
            "Epoch 10/900\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 2122.3120 - val_loss: 2127.4353\n",
            "Epoch 11/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2122.1558 - val_loss: 2127.2749\n",
            "Epoch 12/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2121.9893 - val_loss: 2127.1038\n",
            "Epoch 13/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2121.8105 - val_loss: 2126.9219\n",
            "Epoch 14/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2121.6206 - val_loss: 2126.7278\n",
            "Epoch 15/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2121.4187 - val_loss: 2126.5205\n",
            "Epoch 16/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2121.2026 - val_loss: 2126.3000\n",
            "Epoch 17/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2120.9736 - val_loss: 2126.0649\n",
            "Epoch 18/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2120.7288 - val_loss: 2125.8152\n",
            "Epoch 19/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2120.4675 - val_loss: 2125.5498\n",
            "Epoch 20/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2120.1912 - val_loss: 2125.2693\n",
            "Epoch 21/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2119.8982 - val_loss: 2124.9734\n",
            "Epoch 22/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2119.5901 - val_loss: 2124.6599\n",
            "Epoch 23/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 2119.2634 - val_loss: 2124.3289\n",
            "Epoch 24/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2118.9202 - val_loss: 2123.9783\n",
            "Epoch 25/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2118.5557 - val_loss: 2123.6082\n",
            "Epoch 26/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2118.1697 - val_loss: 2123.2185\n",
            "Epoch 27/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2117.7649 - val_loss: 2122.8064\n",
            "Epoch 28/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 2117.3386 - val_loss: 2122.3706\n",
            "Epoch 29/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2116.8877 - val_loss: 2121.9141\n",
            "Epoch 30/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2116.4170 - val_loss: 2121.4370\n",
            "Epoch 31/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2115.9219 - val_loss: 2120.9348\n",
            "Epoch 32/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2115.4036 - val_loss: 2120.4043\n",
            "Epoch 33/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2114.8540 - val_loss: 2119.8428\n",
            "Epoch 34/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2114.2676 - val_loss: 2119.2278\n",
            "Epoch 35/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 2113.6248 - val_loss: 2118.5627\n",
            "Epoch 36/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 2112.9348 - val_loss: 2117.8555\n",
            "Epoch 37/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2112.2041 - val_loss: 2117.1116\n",
            "Epoch 38/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2111.4363 - val_loss: 2116.3342\n",
            "Epoch 39/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 2110.6340 - val_loss: 2115.5222\n",
            "Epoch 40/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2109.7952 - val_loss: 2114.6775\n",
            "Epoch 41/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2108.9290 - val_loss: 2113.7971\n",
            "Epoch 42/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2108.0251 - val_loss: 2112.8835\n",
            "Epoch 43/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 2107.0837 - val_loss: 2111.9333\n",
            "Epoch 44/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2106.1038 - val_loss: 2110.9431\n",
            "Epoch 45/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2105.0886 - val_loss: 2109.9131\n",
            "Epoch 46/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2104.0249 - val_loss: 2108.8411\n",
            "Epoch 47/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2102.9238 - val_loss: 2107.7290\n",
            "Epoch 48/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2101.7820 - val_loss: 2106.5764\n",
            "Epoch 49/900\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 2100.6013 - val_loss: 2105.3811\n",
            "Epoch 50/900\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 2099.3689 - val_loss: 2104.1462\n",
            "Epoch 51/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 2098.1045 - val_loss: 2102.8643\n",
            "Epoch 52/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 2096.7856 - val_loss: 2101.5386\n",
            "Epoch 53/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2095.4277 - val_loss: 2100.1633\n",
            "Epoch 54/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2094.0164 - val_loss: 2098.7410\n",
            "Epoch 55/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2092.5586 - val_loss: 2097.2686\n",
            "Epoch 56/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 2091.0442 - val_loss: 2095.7500\n",
            "Epoch 57/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2089.4871 - val_loss: 2094.1863\n",
            "Epoch 58/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2087.8843 - val_loss: 2092.5701\n",
            "Epoch 59/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 2086.2249 - val_loss: 2090.8960\n",
            "Epoch 60/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2084.5171 - val_loss: 2089.1667\n",
            "Epoch 61/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 2082.7456 - val_loss: 2087.3979\n",
            "Epoch 62/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 2080.9341 - val_loss: 2085.5706\n",
            "Epoch 63/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2079.0591 - val_loss: 2083.6833\n",
            "Epoch 64/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2077.1233 - val_loss: 2081.7432\n",
            "Epoch 65/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2075.1455 - val_loss: 2079.7485\n",
            "Epoch 66/900\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 2073.1021 - val_loss: 2077.7085\n",
            "Epoch 67/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2071.0027 - val_loss: 2075.6003\n",
            "Epoch 68/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2068.8545 - val_loss: 2073.4275\n",
            "Epoch 69/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2066.6323 - val_loss: 2071.1980\n",
            "Epoch 70/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2064.3494 - val_loss: 2068.9170\n",
            "Epoch 71/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 2062.0176 - val_loss: 2066.5789\n",
            "Epoch 72/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2059.6301 - val_loss: 2064.1733\n",
            "Epoch 73/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 2057.1699 - val_loss: 2061.6975\n",
            "Epoch 74/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 2054.6428 - val_loss: 2059.1575\n",
            "Epoch 75/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 2052.0430 - val_loss: 2056.5500\n",
            "Epoch 76/900\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 2049.3770 - val_loss: 2053.8562\n",
            "Epoch 77/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 2046.6124 - val_loss: 2051.0471\n",
            "Epoch 78/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 2043.7209 - val_loss: 2048.1208\n",
            "Epoch 79/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 2040.7294 - val_loss: 2045.0756\n",
            "Epoch 80/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 2037.6141 - val_loss: 2041.9259\n",
            "Epoch 81/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 2034.3760 - val_loss: 2038.6788\n",
            "Epoch 82/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 2031.0657 - val_loss: 2035.3248\n",
            "Epoch 83/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 2027.6360 - val_loss: 2031.8884\n",
            "Epoch 84/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 2024.1172 - val_loss: 2028.3402\n",
            "Epoch 85/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 2020.4894 - val_loss: 2024.6875\n",
            "Epoch 86/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2016.7646 - val_loss: 2020.9498\n",
            "Epoch 87/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 2012.9586 - val_loss: 2017.1119\n",
            "Epoch 88/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 2009.0291 - val_loss: 2013.1781\n",
            "Epoch 89/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 2005.0287 - val_loss: 2009.1400\n",
            "Epoch 90/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2000.9122 - val_loss: 2005.0176\n",
            "Epoch 91/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1996.7032 - val_loss: 2000.8143\n",
            "Epoch 92/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1992.4021 - val_loss: 1996.5166\n",
            "Epoch 93/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1988.0254 - val_loss: 1992.1121\n",
            "Epoch 94/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1983.5295 - val_loss: 1987.6106\n",
            "Epoch 95/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1978.9446 - val_loss: 1982.9985\n",
            "Epoch 96/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1974.2646 - val_loss: 1978.2738\n",
            "Epoch 97/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 1969.4421 - val_loss: 1973.4843\n",
            "Epoch 98/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1964.5668 - val_loss: 1968.5948\n",
            "Epoch 99/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 1959.5864 - val_loss: 1963.5980\n",
            "Epoch 100/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1954.4927 - val_loss: 1958.4814\n",
            "Epoch 101/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1949.2522 - val_loss: 1953.2606\n",
            "Epoch 102/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1943.9193 - val_loss: 1947.9105\n",
            "Epoch 103/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1938.4854 - val_loss: 1942.4393\n",
            "Epoch 104/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1932.9041 - val_loss: 1936.9016\n",
            "Epoch 105/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1927.2787 - val_loss: 1931.2623\n",
            "Epoch 106/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1921.5548 - val_loss: 1925.5151\n",
            "Epoch 107/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1915.6772 - val_loss: 1919.6759\n",
            "Epoch 108/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 1909.7469 - val_loss: 1913.7043\n",
            "Epoch 109/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1903.6383 - val_loss: 1907.6194\n",
            "Epoch 110/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1897.4702 - val_loss: 1901.4353\n",
            "Epoch 111/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1891.1534 - val_loss: 1895.1562\n",
            "Epoch 112/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1884.7985 - val_loss: 1888.7545\n",
            "Epoch 113/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1878.2858 - val_loss: 1882.2511\n",
            "Epoch 114/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1871.6525 - val_loss: 1875.6609\n",
            "Epoch 115/900\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 1864.9609 - val_loss: 1868.9536\n",
            "Epoch 116/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1858.1342 - val_loss: 1862.1696\n",
            "Epoch 117/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1851.2290 - val_loss: 1855.2725\n",
            "Epoch 118/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1844.2212 - val_loss: 1848.2430\n",
            "Epoch 119/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1837.0649 - val_loss: 1841.0859\n",
            "Epoch 120/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1829.7773 - val_loss: 1833.8423\n",
            "Epoch 121/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1822.4020 - val_loss: 1826.4469\n",
            "Epoch 122/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1814.8821 - val_loss: 1818.9452\n",
            "Epoch 123/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1807.2457 - val_loss: 1811.3500\n",
            "Epoch 124/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 1799.5522 - val_loss: 1803.6050\n",
            "Epoch 125/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 1791.6366 - val_loss: 1795.7719\n",
            "Epoch 126/900\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 1783.7194 - val_loss: 1787.8076\n",
            "Epoch 127/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1775.5726 - val_loss: 1779.7246\n",
            "Epoch 128/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1767.3716 - val_loss: 1771.5063\n",
            "Epoch 129/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1759.0232 - val_loss: 1763.2086\n",
            "Epoch 130/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1750.5631 - val_loss: 1754.7422\n",
            "Epoch 131/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1741.9225 - val_loss: 1746.1770\n",
            "Epoch 132/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1733.2472 - val_loss: 1737.4319\n",
            "Epoch 133/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1724.3610 - val_loss: 1728.5920\n",
            "Epoch 134/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 1715.3612 - val_loss: 1719.6362\n",
            "Epoch 135/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1706.2559 - val_loss: 1710.5555\n",
            "Epoch 136/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1697.0186 - val_loss: 1701.3612\n",
            "Epoch 137/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1687.7229 - val_loss: 1692.0275\n",
            "Epoch 138/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1678.2289 - val_loss: 1682.6130\n",
            "Epoch 139/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1668.6565 - val_loss: 1673.0505\n",
            "Epoch 140/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1658.9578 - val_loss: 1663.4023\n",
            "Epoch 141/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1649.1293 - val_loss: 1653.5797\n",
            "Epoch 142/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1639.1604 - val_loss: 1643.6523\n",
            "Epoch 143/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1629.1074 - val_loss: 1633.5896\n",
            "Epoch 144/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1618.8740 - val_loss: 1623.4987\n",
            "Epoch 145/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 1608.6222 - val_loss: 1613.2594\n",
            "Epoch 146/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 1598.2078 - val_loss: 1602.8451\n",
            "Epoch 147/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1587.6127 - val_loss: 1592.2987\n",
            "Epoch 148/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1576.9406 - val_loss: 1581.6244\n",
            "Epoch 149/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 1566.0757 - val_loss: 1570.8477\n",
            "Epoch 150/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 1555.1202 - val_loss: 1559.9186\n",
            "Epoch 151/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1544.0363 - val_loss: 1548.8048\n",
            "Epoch 152/900\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 1532.7294 - val_loss: 1537.6136\n",
            "Epoch 153/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1521.3616 - val_loss: 1526.2477\n",
            "Epoch 154/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1509.8038 - val_loss: 1514.7493\n",
            "Epoch 155/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1498.1522 - val_loss: 1503.1228\n",
            "Epoch 156/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1486.3110 - val_loss: 1491.4059\n",
            "Epoch 157/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 1474.4309 - val_loss: 1479.5042\n",
            "Epoch 158/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 1462.3315 - val_loss: 1467.4468\n",
            "Epoch 159/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1450.1079 - val_loss: 1455.2882\n",
            "Epoch 160/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1437.7542 - val_loss: 1443.0038\n",
            "Epoch 161/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1425.2919 - val_loss: 1430.5919\n",
            "Epoch 162/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 1412.6813 - val_loss: 1418.0245\n",
            "Epoch 163/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 1399.9117 - val_loss: 1405.2932\n",
            "Epoch 164/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1386.9545 - val_loss: 1392.4427\n",
            "Epoch 165/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1373.9246 - val_loss: 1379.4014\n",
            "Epoch 166/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1360.7236 - val_loss: 1366.2646\n",
            "Epoch 167/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1347.4425 - val_loss: 1353.0538\n",
            "Epoch 168/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1334.0398 - val_loss: 1339.7197\n",
            "Epoch 169/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1320.5452 - val_loss: 1326.1722\n",
            "Epoch 170/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1306.8514 - val_loss: 1312.4449\n",
            "Epoch 171/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1293.0143 - val_loss: 1298.6447\n",
            "Epoch 172/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1279.0850 - val_loss: 1284.7092\n",
            "Epoch 173/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1265.0321 - val_loss: 1270.6581\n",
            "Epoch 174/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1250.7753 - val_loss: 1256.4694\n",
            "Epoch 175/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 1236.3728 - val_loss: 1242.0887\n",
            "Epoch 176/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1221.8590 - val_loss: 1227.5006\n",
            "Epoch 177/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1207.0867 - val_loss: 1212.7972\n",
            "Epoch 178/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 1192.2812 - val_loss: 1197.9656\n",
            "Epoch 179/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1177.3108 - val_loss: 1183.0067\n",
            "Epoch 180/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1162.1675 - val_loss: 1167.9639\n",
            "Epoch 181/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1146.9287 - val_loss: 1152.7804\n",
            "Epoch 182/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1131.6539 - val_loss: 1137.4293\n",
            "Epoch 183/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1116.0986 - val_loss: 1121.9969\n",
            "Epoch 184/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1100.6010 - val_loss: 1106.4037\n",
            "Epoch 185/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1084.9430 - val_loss: 1090.6901\n",
            "Epoch 186/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1069.1080 - val_loss: 1074.8866\n",
            "Epoch 187/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1053.2094 - val_loss: 1058.8715\n",
            "Epoch 188/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1037.0331 - val_loss: 1042.7323\n",
            "Epoch 189/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1020.9545 - val_loss: 1026.4493\n",
            "Epoch 190/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1004.6377 - val_loss: 1010.3665\n",
            "Epoch 191/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 988.3315 - val_loss: 994.1724\n",
            "Epoch 192/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 971.6302 - val_loss: 977.8595\n",
            "Epoch 193/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 955.2531 - val_loss: 961.4340\n",
            "Epoch 194/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 939.0925 - val_loss: 945.0532\n",
            "Epoch 195/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 923.1633 - val_loss: 928.7822\n",
            "Epoch 196/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 907.5478 - val_loss: 912.7409\n",
            "Epoch 197/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 892.7339 - val_loss: 896.8776\n",
            "Epoch 198/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 877.9355 - val_loss: 881.3063\n",
            "Epoch 199/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 863.6808 - val_loss: 865.7070\n",
            "Epoch 200/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 849.3521 - val_loss: 850.1992\n",
            "Epoch 201/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 835.1835 - val_loss: 834.8669\n",
            "Epoch 202/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 821.6071 - val_loss: 819.3218\n",
            "Epoch 203/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 807.7642 - val_loss: 804.5056\n",
            "Epoch 204/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 794.6603 - val_loss: 790.2899\n",
            "Epoch 205/900\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 781.8687 - val_loss: 777.0461\n",
            "Epoch 206/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 769.3402 - val_loss: 763.9020\n",
            "Epoch 207/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 757.0728 - val_loss: 750.8557\n",
            "Epoch 208/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 745.2583 - val_loss: 738.5169\n",
            "Epoch 209/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 734.4924 - val_loss: 726.9001\n",
            "Epoch 210/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 724.4404 - val_loss: 715.7966\n",
            "Epoch 211/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 715.2590 - val_loss: 704.9476\n",
            "Epoch 212/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 706.5500 - val_loss: 694.4280\n",
            "Epoch 213/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 698.2670 - val_loss: 684.5740\n",
            "Epoch 214/900\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 690.8513 - val_loss: 675.4748\n",
            "Epoch 215/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 683.6706 - val_loss: 666.9987\n",
            "Epoch 216/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 676.7505 - val_loss: 659.0165\n",
            "Epoch 217/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 670.1427 - val_loss: 651.0489\n",
            "Epoch 218/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 663.8685 - val_loss: 643.1284\n",
            "Epoch 219/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 657.7820 - val_loss: 635.5424\n",
            "Epoch 220/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 651.5739 - val_loss: 628.2561\n",
            "Epoch 221/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 645.5618 - val_loss: 620.9964\n",
            "Epoch 222/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 639.6277 - val_loss: 613.7351\n",
            "Epoch 223/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 633.8054 - val_loss: 606.7579\n",
            "Epoch 224/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 627.8743 - val_loss: 599.9859\n",
            "Epoch 225/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 622.3262 - val_loss: 593.3847\n",
            "Epoch 226/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 616.9376 - val_loss: 586.9918\n",
            "Epoch 227/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 611.5133 - val_loss: 580.7040\n",
            "Epoch 228/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 606.2809 - val_loss: 574.5474\n",
            "Epoch 229/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 601.2500 - val_loss: 568.3453\n",
            "Epoch 230/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 596.1397 - val_loss: 562.5981\n",
            "Epoch 231/900\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 591.1362 - val_loss: 557.2535\n",
            "Epoch 232/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 586.2483 - val_loss: 551.8872\n",
            "Epoch 233/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 581.4339 - val_loss: 546.7479\n",
            "Epoch 234/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 577.0361 - val_loss: 541.7225\n",
            "Epoch 235/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 572.5934 - val_loss: 536.8609\n",
            "Epoch 236/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 568.4512 - val_loss: 532.0536\n",
            "Epoch 237/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 564.3605 - val_loss: 527.5091\n",
            "Epoch 238/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 560.5156 - val_loss: 523.3046\n",
            "Epoch 239/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 556.7413 - val_loss: 519.3550\n",
            "Epoch 240/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 553.3354 - val_loss: 515.5720\n",
            "Epoch 241/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 549.8881 - val_loss: 511.9958\n",
            "Epoch 242/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 546.7710 - val_loss: 508.5463\n",
            "Epoch 243/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 543.4955 - val_loss: 505.4581\n",
            "Epoch 244/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 540.6960 - val_loss: 502.2274\n",
            "Epoch 245/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 537.5637 - val_loss: 499.0766\n",
            "Epoch 246/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 534.5816 - val_loss: 495.9532\n",
            "Epoch 247/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 531.4750 - val_loss: 492.9254\n",
            "Epoch 248/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 528.4681 - val_loss: 489.8235\n",
            "Epoch 249/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 525.5152 - val_loss: 486.8865\n",
            "Epoch 250/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 522.7363 - val_loss: 484.2796\n",
            "Epoch 251/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 519.9476 - val_loss: 481.9228\n",
            "Epoch 252/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 517.4531 - val_loss: 479.5375\n",
            "Epoch 253/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 514.8206 - val_loss: 477.3331\n",
            "Epoch 254/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 512.2711 - val_loss: 475.2291\n",
            "Epoch 255/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 509.9093 - val_loss: 473.1677\n",
            "Epoch 256/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 507.6405 - val_loss: 471.2401\n",
            "Epoch 257/900\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 505.5829 - val_loss: 469.4403\n",
            "Epoch 258/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 503.5047 - val_loss: 467.8050\n",
            "Epoch 259/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 501.8344 - val_loss: 466.0902\n",
            "Epoch 260/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 500.0579 - val_loss: 464.4913\n",
            "Epoch 261/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 498.3751 - val_loss: 462.8926\n",
            "Epoch 262/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 496.7783 - val_loss: 461.2354\n",
            "Epoch 263/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 495.1523 - val_loss: 459.6558\n",
            "Epoch 264/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 493.5417 - val_loss: 458.1935\n",
            "Epoch 265/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 492.1093 - val_loss: 456.6575\n",
            "Epoch 266/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 490.6114 - val_loss: 455.1931\n",
            "Epoch 267/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 489.1214 - val_loss: 453.7826\n",
            "Epoch 268/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 487.7444 - val_loss: 452.3813\n",
            "Epoch 269/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 486.2881 - val_loss: 451.0317\n",
            "Epoch 270/900\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 484.8708 - val_loss: 449.7354\n",
            "Epoch 271/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 483.5408 - val_loss: 448.4758\n",
            "Epoch 272/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 482.1580 - val_loss: 447.2169\n",
            "Epoch 273/900\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 480.7417 - val_loss: 445.9415\n",
            "Epoch 274/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 479.4102 - val_loss: 444.6743\n",
            "Epoch 275/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 478.0284 - val_loss: 443.4604\n",
            "Epoch 276/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 476.7147 - val_loss: 442.2045\n",
            "Epoch 277/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 475.3385 - val_loss: 440.9435\n",
            "Epoch 278/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 473.9762 - val_loss: 439.6674\n",
            "Epoch 279/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 472.6334 - val_loss: 438.4277\n",
            "Epoch 280/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 471.3170 - val_loss: 437.2582\n",
            "Epoch 281/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 470.0572 - val_loss: 436.0572\n",
            "Epoch 282/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 468.7358 - val_loss: 434.8328\n",
            "Epoch 283/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 467.4839 - val_loss: 433.5495\n",
            "Epoch 284/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 466.1787 - val_loss: 432.3155\n",
            "Epoch 285/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 464.8625 - val_loss: 431.1509\n",
            "Epoch 286/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 463.5988 - val_loss: 430.0165\n",
            "Epoch 287/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 462.3767 - val_loss: 428.8797\n",
            "Epoch 288/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 461.1050 - val_loss: 427.7411\n",
            "Epoch 289/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 459.8686 - val_loss: 426.5622\n",
            "Epoch 290/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 458.5495 - val_loss: 425.4298\n",
            "Epoch 291/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 457.3242 - val_loss: 424.2698\n",
            "Epoch 292/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 456.0890 - val_loss: 423.1325\n",
            "Epoch 293/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 454.8656 - val_loss: 422.0081\n",
            "Epoch 294/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 453.6511 - val_loss: 420.8803\n",
            "Epoch 295/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 452.4493 - val_loss: 419.7312\n",
            "Epoch 296/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 451.2764 - val_loss: 418.6014\n",
            "Epoch 297/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 450.0907 - val_loss: 417.5103\n",
            "Epoch 298/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 448.9463 - val_loss: 416.3753\n",
            "Epoch 299/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 447.7331 - val_loss: 415.3060\n",
            "Epoch 300/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 446.5935 - val_loss: 414.2280\n",
            "Epoch 301/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 445.4299 - val_loss: 413.1296\n",
            "Epoch 302/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 444.2659 - val_loss: 411.9812\n",
            "Epoch 303/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 443.0868 - val_loss: 410.8563\n",
            "Epoch 304/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 441.9362 - val_loss: 409.7222\n",
            "Epoch 305/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 440.7605 - val_loss: 408.6701\n",
            "Epoch 306/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 439.6365 - val_loss: 407.6178\n",
            "Epoch 307/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 438.4960 - val_loss: 406.5674\n",
            "Epoch 308/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 437.3495 - val_loss: 405.4904\n",
            "Epoch 309/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 436.1954 - val_loss: 404.4779\n",
            "Epoch 310/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 435.0692 - val_loss: 403.4226\n",
            "Epoch 311/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 433.9132 - val_loss: 402.3613\n",
            "Epoch 312/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 432.7730 - val_loss: 401.2821\n",
            "Epoch 313/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 431.6242 - val_loss: 400.1868\n",
            "Epoch 314/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 430.4642 - val_loss: 399.1346\n",
            "Epoch 315/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 429.3469 - val_loss: 398.1452\n",
            "Epoch 316/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 428.2222 - val_loss: 397.1086\n",
            "Epoch 317/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 427.1132 - val_loss: 396.0532\n",
            "Epoch 318/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 425.9566 - val_loss: 395.0210\n",
            "Epoch 319/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 424.8285 - val_loss: 393.9285\n",
            "Epoch 320/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 423.6525 - val_loss: 392.8503\n",
            "Epoch 321/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 422.5131 - val_loss: 391.8282\n",
            "Epoch 322/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 421.4032 - val_loss: 390.8517\n",
            "Epoch 323/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 420.2675 - val_loss: 389.8220\n",
            "Epoch 324/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 419.1437 - val_loss: 388.8115\n",
            "Epoch 325/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 418.0166 - val_loss: 387.7859\n",
            "Epoch 326/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 416.8733 - val_loss: 386.7163\n",
            "Epoch 327/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 415.7427 - val_loss: 385.5787\n",
            "Epoch 328/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 414.5979 - val_loss: 384.5294\n",
            "Epoch 329/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 413.4693 - val_loss: 383.4830\n",
            "Epoch 330/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 412.3494 - val_loss: 382.5007\n",
            "Epoch 331/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 411.2442 - val_loss: 381.4969\n",
            "Epoch 332/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 410.1198 - val_loss: 380.3683\n",
            "Epoch 333/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 408.9535 - val_loss: 379.3518\n",
            "Epoch 334/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 407.8372 - val_loss: 378.3453\n",
            "Epoch 335/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 406.7399 - val_loss: 377.2457\n",
            "Epoch 336/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 405.5854 - val_loss: 376.2507\n",
            "Epoch 337/900\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 404.5008 - val_loss: 375.2332\n",
            "Epoch 338/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 403.3873 - val_loss: 374.2212\n",
            "Epoch 339/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 402.2631 - val_loss: 373.2064\n",
            "Epoch 340/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 401.1657 - val_loss: 372.2691\n",
            "Epoch 341/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 400.0731 - val_loss: 371.2439\n",
            "Epoch 342/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 398.9937 - val_loss: 370.2241\n",
            "Epoch 343/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 397.9100 - val_loss: 369.2829\n",
            "Epoch 344/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 396.8271 - val_loss: 368.3084\n",
            "Epoch 345/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 395.7311 - val_loss: 367.3698\n",
            "Epoch 346/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 394.6184 - val_loss: 366.4749\n",
            "Epoch 347/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 393.5277 - val_loss: 365.4689\n",
            "Epoch 348/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 392.4352 - val_loss: 364.4618\n",
            "Epoch 349/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 391.3433 - val_loss: 363.4628\n",
            "Epoch 350/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 390.2750 - val_loss: 362.4527\n",
            "Epoch 351/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 389.1740 - val_loss: 361.4497\n",
            "Epoch 352/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 388.1058 - val_loss: 360.4812\n",
            "Epoch 353/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 387.0530 - val_loss: 359.5036\n",
            "Epoch 354/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 385.9951 - val_loss: 358.6172\n",
            "Epoch 355/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 384.9450 - val_loss: 357.7135\n",
            "Epoch 356/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 383.9095 - val_loss: 356.8261\n",
            "Epoch 357/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 382.9044 - val_loss: 355.9268\n",
            "Epoch 358/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 381.8746 - val_loss: 355.0142\n",
            "Epoch 359/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 380.8487 - val_loss: 354.1090\n",
            "Epoch 360/900\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 379.8466 - val_loss: 353.2116\n",
            "Epoch 361/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 378.8312 - val_loss: 352.2393\n",
            "Epoch 362/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 377.7907 - val_loss: 351.2508\n",
            "Epoch 363/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 376.7499 - val_loss: 350.2753\n",
            "Epoch 364/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 375.7672 - val_loss: 349.3072\n",
            "Epoch 365/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 374.7308 - val_loss: 348.3743\n",
            "Epoch 366/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 373.7418 - val_loss: 347.4308\n",
            "Epoch 367/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 372.7018 - val_loss: 346.4858\n",
            "Epoch 368/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 371.6801 - val_loss: 345.5229\n",
            "Epoch 369/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 370.6409 - val_loss: 344.5844\n",
            "Epoch 370/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 369.5715 - val_loss: 343.7019\n",
            "Epoch 371/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 368.5605 - val_loss: 342.7687\n",
            "Epoch 372/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 367.5114 - val_loss: 341.8379\n",
            "Epoch 373/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 366.4855 - val_loss: 340.8569\n",
            "Epoch 374/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 365.4777 - val_loss: 339.8773\n",
            "Epoch 375/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 364.4998 - val_loss: 338.9091\n",
            "Epoch 376/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 363.4908 - val_loss: 338.0471\n",
            "Epoch 377/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 362.5328 - val_loss: 337.1555\n",
            "Epoch 378/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 361.5865 - val_loss: 336.3119\n",
            "Epoch 379/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 360.6141 - val_loss: 335.4320\n",
            "Epoch 380/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 359.6527 - val_loss: 334.5585\n",
            "Epoch 381/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 358.6686 - val_loss: 333.6752\n",
            "Epoch 382/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 357.7405 - val_loss: 332.7573\n",
            "Epoch 383/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 356.8025 - val_loss: 331.9011\n",
            "Epoch 384/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 355.8708 - val_loss: 331.0504\n",
            "Epoch 385/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 354.9784 - val_loss: 330.2046\n",
            "Epoch 386/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 354.0508 - val_loss: 329.3369\n",
            "Epoch 387/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 353.1219 - val_loss: 328.5061\n",
            "Epoch 388/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 352.2265 - val_loss: 327.7060\n",
            "Epoch 389/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 351.3752 - val_loss: 326.9247\n",
            "Epoch 390/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 350.4849 - val_loss: 326.1333\n",
            "Epoch 391/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 349.6306 - val_loss: 325.3321\n",
            "Epoch 392/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 348.7527 - val_loss: 324.5178\n",
            "Epoch 393/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 347.8983 - val_loss: 323.7327\n",
            "Epoch 394/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 347.0090 - val_loss: 322.9350\n",
            "Epoch 395/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 346.1883 - val_loss: 322.1073\n",
            "Epoch 396/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 345.3259 - val_loss: 321.3069\n",
            "Epoch 397/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 344.4560 - val_loss: 320.4724\n",
            "Epoch 398/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 343.6070 - val_loss: 319.6466\n",
            "Epoch 399/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 342.8069 - val_loss: 318.8248\n",
            "Epoch 400/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 341.9574 - val_loss: 318.0372\n",
            "Epoch 401/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 341.1292 - val_loss: 317.2533\n",
            "Epoch 402/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 340.2881 - val_loss: 316.4991\n",
            "Epoch 403/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 339.4847 - val_loss: 315.7448\n",
            "Epoch 404/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 338.6733 - val_loss: 314.9824\n",
            "Epoch 405/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 337.8869 - val_loss: 314.2975\n",
            "Epoch 406/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 337.1142 - val_loss: 313.5889\n",
            "Epoch 407/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 336.3444 - val_loss: 312.8409\n",
            "Epoch 408/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 335.5779 - val_loss: 312.0699\n",
            "Epoch 409/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 334.7880 - val_loss: 311.3006\n",
            "Epoch 410/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 334.0241 - val_loss: 310.5409\n",
            "Epoch 411/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 333.2343 - val_loss: 309.7829\n",
            "Epoch 412/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 332.5206 - val_loss: 309.0087\n",
            "Epoch 413/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 331.7279 - val_loss: 308.2870\n",
            "Epoch 414/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 330.9861 - val_loss: 307.5419\n",
            "Epoch 415/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 330.2006 - val_loss: 306.7792\n",
            "Epoch 416/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 329.4238 - val_loss: 306.0386\n",
            "Epoch 417/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 328.6525 - val_loss: 305.3648\n",
            "Epoch 418/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 327.9143 - val_loss: 304.7208\n",
            "Epoch 419/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 327.1536 - val_loss: 303.9941\n",
            "Epoch 420/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 326.4055 - val_loss: 303.2886\n",
            "Epoch 421/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 325.6874 - val_loss: 302.5483\n",
            "Epoch 422/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 324.9305 - val_loss: 301.8307\n",
            "Epoch 423/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 324.1676 - val_loss: 301.1190\n",
            "Epoch 424/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 323.4254 - val_loss: 300.3939\n",
            "Epoch 425/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 322.6993 - val_loss: 299.6826\n",
            "Epoch 426/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 321.9469 - val_loss: 298.9363\n",
            "Epoch 427/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 321.1990 - val_loss: 298.1826\n",
            "Epoch 428/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 320.4519 - val_loss: 297.4636\n",
            "Epoch 429/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 319.7302 - val_loss: 296.7610\n",
            "Epoch 430/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 318.9928 - val_loss: 296.0819\n",
            "Epoch 431/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 318.2841 - val_loss: 295.3714\n",
            "Epoch 432/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 317.5480 - val_loss: 294.6281\n",
            "Epoch 433/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 316.8291 - val_loss: 293.8737\n",
            "Epoch 434/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 316.0800 - val_loss: 293.1765\n",
            "Epoch 435/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 315.3418 - val_loss: 292.5059\n",
            "Epoch 436/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 314.6219 - val_loss: 291.8900\n",
            "Epoch 437/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 313.8986 - val_loss: 291.2935\n",
            "Epoch 438/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 313.1941 - val_loss: 290.7468\n",
            "Epoch 439/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 312.5851 - val_loss: 290.2279\n",
            "Epoch 440/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 311.8830 - val_loss: 289.5518\n",
            "Epoch 441/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 311.1853 - val_loss: 288.8811\n",
            "Epoch 442/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 310.4973 - val_loss: 288.1277\n",
            "Epoch 443/900\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 309.8138 - val_loss: 287.4639\n",
            "Epoch 444/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 309.1188 - val_loss: 286.8197\n",
            "Epoch 445/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 308.4234 - val_loss: 286.1891\n",
            "Epoch 446/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 307.7406 - val_loss: 285.4912\n",
            "Epoch 447/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 307.0969 - val_loss: 284.8348\n",
            "Epoch 448/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 306.4030 - val_loss: 284.1752\n",
            "Epoch 449/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 305.7375 - val_loss: 283.5710\n",
            "Epoch 450/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 305.0886 - val_loss: 283.0198\n",
            "Epoch 451/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 304.4400 - val_loss: 282.4119\n",
            "Epoch 452/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 303.8007 - val_loss: 281.7930\n",
            "Epoch 453/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 303.1726 - val_loss: 281.1336\n",
            "Epoch 454/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 302.5461 - val_loss: 280.4787\n",
            "Epoch 455/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 301.9442 - val_loss: 279.9070\n",
            "Epoch 456/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 301.3332 - val_loss: 279.3181\n",
            "Epoch 457/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 300.7089 - val_loss: 278.7826\n",
            "Epoch 458/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 300.1033 - val_loss: 278.3097\n",
            "Epoch 459/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 299.4830 - val_loss: 277.7893\n",
            "Epoch 460/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 298.9043 - val_loss: 277.2819\n",
            "Epoch 461/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 298.2925 - val_loss: 276.6991\n",
            "Epoch 462/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 297.6900 - val_loss: 276.0680\n",
            "Epoch 463/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 297.1133 - val_loss: 275.5071\n",
            "Epoch 464/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 296.4863 - val_loss: 274.9026\n",
            "Epoch 465/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 295.9169 - val_loss: 274.3381\n",
            "Epoch 466/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 295.3389 - val_loss: 273.7183\n",
            "Epoch 467/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 294.7717 - val_loss: 273.1617\n",
            "Epoch 468/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 294.2257 - val_loss: 272.5686\n",
            "Epoch 469/900\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 293.6840 - val_loss: 271.9150\n",
            "Epoch 470/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 293.1091 - val_loss: 271.2430\n",
            "Epoch 471/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 292.5787 - val_loss: 270.5618\n",
            "Epoch 472/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 292.0409 - val_loss: 269.9738\n",
            "Epoch 473/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 291.4652 - val_loss: 269.5674\n",
            "Epoch 474/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 290.9303 - val_loss: 269.1020\n",
            "Epoch 475/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 290.4219 - val_loss: 268.6859\n",
            "Epoch 476/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 289.9000 - val_loss: 268.1313\n",
            "Epoch 477/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 289.3633 - val_loss: 267.5464\n",
            "Epoch 478/900\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 288.8996 - val_loss: 267.0437\n",
            "Epoch 479/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 288.3964 - val_loss: 266.6811\n",
            "Epoch 480/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 287.9346 - val_loss: 266.2574\n",
            "Epoch 481/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 287.4501 - val_loss: 265.7567\n",
            "Epoch 482/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 286.9763 - val_loss: 265.3959\n",
            "Epoch 483/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 286.4752 - val_loss: 264.9423\n",
            "Epoch 484/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 286.0062 - val_loss: 264.4584\n",
            "Epoch 485/900\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 285.5535 - val_loss: 263.9425\n",
            "Epoch 486/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 285.0780 - val_loss: 263.4513\n",
            "Epoch 487/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 284.6299 - val_loss: 263.1144\n",
            "Epoch 488/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 284.1979 - val_loss: 262.7977\n",
            "Epoch 489/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 283.7194 - val_loss: 262.3366\n",
            "Epoch 490/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 283.2851 - val_loss: 261.8251\n",
            "Epoch 491/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 282.8441 - val_loss: 261.2897\n",
            "Epoch 492/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 282.4145 - val_loss: 260.8675\n",
            "Epoch 493/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 281.9539 - val_loss: 260.5884\n",
            "Epoch 494/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 281.5186 - val_loss: 260.2732\n",
            "Epoch 495/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 281.0570 - val_loss: 259.9651\n",
            "Epoch 496/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 280.6207 - val_loss: 259.6701\n",
            "Epoch 497/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 280.2174 - val_loss: 259.1571\n",
            "Epoch 498/900\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 279.7429 - val_loss: 258.6908\n",
            "Epoch 499/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 279.3035 - val_loss: 258.1803\n",
            "Epoch 500/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 278.9082 - val_loss: 257.6841\n",
            "Epoch 501/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 278.4768 - val_loss: 257.2663\n",
            "Epoch 502/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 278.1085 - val_loss: 256.8145\n",
            "Epoch 503/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 277.6401 - val_loss: 256.5072\n",
            "Epoch 504/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 277.2378 - val_loss: 256.2153\n",
            "Epoch 505/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 276.8012 - val_loss: 255.9110\n",
            "Epoch 506/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 276.3649 - val_loss: 255.5417\n",
            "Epoch 507/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 275.9478 - val_loss: 255.1485\n",
            "Epoch 508/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 275.5503 - val_loss: 254.8937\n",
            "Epoch 509/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 275.0856 - val_loss: 254.5370\n",
            "Epoch 510/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 274.6973 - val_loss: 254.2048\n",
            "Epoch 511/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 274.2868 - val_loss: 253.7203\n",
            "Epoch 512/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 273.9060 - val_loss: 253.3937\n",
            "Epoch 513/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 273.5115 - val_loss: 253.0823\n",
            "Epoch 514/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 273.1429 - val_loss: 252.7795\n",
            "Epoch 515/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 272.7479 - val_loss: 252.4908\n",
            "Epoch 516/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 272.3974 - val_loss: 252.3507\n",
            "Epoch 517/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 271.9996 - val_loss: 252.1747\n",
            "Epoch 518/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 271.6277 - val_loss: 251.9633\n",
            "Epoch 519/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 271.2473 - val_loss: 251.5893\n",
            "Epoch 520/900\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 270.8672 - val_loss: 251.3411\n",
            "Epoch 521/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 270.4725 - val_loss: 250.9523\n",
            "Epoch 522/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 270.1057 - val_loss: 250.6486\n",
            "Epoch 523/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 269.7310 - val_loss: 250.3892\n",
            "Epoch 524/900\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 269.3615 - val_loss: 250.0048\n",
            "Epoch 525/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 268.9967 - val_loss: 249.7950\n",
            "Epoch 526/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 268.6534 - val_loss: 249.5393\n",
            "Epoch 527/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 268.3135 - val_loss: 249.4650\n",
            "Epoch 528/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 267.9434 - val_loss: 249.3901\n",
            "Epoch 529/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 267.6071 - val_loss: 249.1503\n",
            "Epoch 530/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 267.2774 - val_loss: 248.8974\n",
            "Epoch 531/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 266.9891 - val_loss: 248.9889\n",
            "Epoch 532/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 266.6071 - val_loss: 248.7411\n",
            "Epoch 533/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 266.2766 - val_loss: 248.3223\n",
            "Epoch 534/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 265.9852 - val_loss: 247.7644\n",
            "Epoch 535/900\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 265.6425 - val_loss: 247.6410\n",
            "Epoch 536/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 265.3008 - val_loss: 247.4938\n",
            "Epoch 537/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 264.9678 - val_loss: 247.3902\n",
            "Epoch 538/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 264.6389 - val_loss: 247.2934\n",
            "Epoch 539/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 264.3174 - val_loss: 247.0959\n",
            "Epoch 540/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 264.0176 - val_loss: 246.9246\n",
            "Epoch 541/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 263.7415 - val_loss: 246.8981\n",
            "Epoch 542/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 263.4335 - val_loss: 246.5771\n",
            "Epoch 543/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 263.1344 - val_loss: 246.3971\n",
            "Epoch 544/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 262.7932 - val_loss: 246.0137\n",
            "Epoch 545/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 262.4829 - val_loss: 245.6930\n",
            "Epoch 546/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 262.1622 - val_loss: 245.5219\n",
            "Epoch 547/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 261.8736 - val_loss: 245.3716\n",
            "Epoch 548/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 261.5764 - val_loss: 245.0166\n",
            "Epoch 549/900\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 261.2641 - val_loss: 244.7595\n",
            "Epoch 550/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 260.9474 - val_loss: 244.3662\n",
            "Epoch 551/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 260.6728 - val_loss: 244.0595\n",
            "Epoch 552/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 260.3908 - val_loss: 243.7620\n",
            "Epoch 553/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 260.0776 - val_loss: 243.7100\n",
            "Epoch 554/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 259.7705 - val_loss: 243.5963\n",
            "Epoch 555/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 259.4867 - val_loss: 243.4480\n",
            "Epoch 556/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 259.1779 - val_loss: 243.0377\n",
            "Epoch 557/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 258.8932 - val_loss: 242.6432\n",
            "Epoch 558/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 258.5787 - val_loss: 242.3523\n",
            "Epoch 559/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 258.3168 - val_loss: 242.0271\n",
            "Epoch 560/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 258.0364 - val_loss: 241.8092\n",
            "Epoch 561/900\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 257.7487 - val_loss: 241.7540\n",
            "Epoch 562/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 257.4366 - val_loss: 241.5267\n",
            "Epoch 563/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 257.1437 - val_loss: 241.2903\n",
            "Epoch 564/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 256.8495 - val_loss: 241.0385\n",
            "Epoch 565/900\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 256.5509 - val_loss: 240.7668\n",
            "Epoch 566/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 256.2618 - val_loss: 240.4891\n",
            "Epoch 567/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 255.9691 - val_loss: 240.3970\n",
            "Epoch 568/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 255.6636 - val_loss: 240.3019\n",
            "Epoch 569/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 255.3680 - val_loss: 240.1232\n",
            "Epoch 570/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 255.0795 - val_loss: 240.0520\n",
            "Epoch 571/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 254.7861 - val_loss: 239.9115\n",
            "Epoch 572/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 254.4922 - val_loss: 239.7480\n",
            "Epoch 573/900\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 254.2535 - val_loss: 239.7810\n",
            "Epoch 574/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 253.9294 - val_loss: 239.5501\n",
            "Epoch 575/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 253.6427 - val_loss: 239.4150\n",
            "Epoch 576/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 253.3611 - val_loss: 239.1570\n",
            "Epoch 577/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 253.1093 - val_loss: 239.0164\n",
            "Epoch 578/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 252.8212 - val_loss: 239.0137\n",
            "Epoch 579/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 252.5211 - val_loss: 239.0522\n",
            "Epoch 580/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 252.2934 - val_loss: 239.0060\n",
            "Epoch 581/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 252.0579 - val_loss: 238.9520\n",
            "Epoch 582/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 251.7834 - val_loss: 238.5715\n",
            "Epoch 583/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 251.4976 - val_loss: 238.1092\n",
            "Epoch 584/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 251.2759 - val_loss: 237.7126\n",
            "Epoch 585/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 250.9999 - val_loss: 237.5631\n",
            "Epoch 586/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 250.7420 - val_loss: 237.2095\n",
            "Epoch 587/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 250.5021 - val_loss: 236.9913\n",
            "Epoch 588/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 250.2517 - val_loss: 236.7114\n",
            "Epoch 589/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 250.0026 - val_loss: 236.5286\n",
            "Epoch 590/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 249.7672 - val_loss: 236.3647\n",
            "Epoch 591/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 249.5101 - val_loss: 236.3672\n",
            "Epoch 592/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 249.2615 - val_loss: 236.5255\n",
            "Epoch 593/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 249.0236 - val_loss: 236.4580\n",
            "Epoch 594/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 248.8162 - val_loss: 236.3417\n",
            "Epoch 595/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 248.5842 - val_loss: 236.1301\n",
            "Epoch 596/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 248.3544 - val_loss: 235.8984\n",
            "Epoch 597/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 248.1138 - val_loss: 235.5748\n",
            "Epoch 598/900\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 247.8962 - val_loss: 235.1510\n",
            "Epoch 599/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 247.6599 - val_loss: 234.8369\n",
            "Epoch 600/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 247.4382 - val_loss: 234.4853\n",
            "Epoch 601/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 247.2202 - val_loss: 234.1134\n",
            "Epoch 602/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 247.0140 - val_loss: 233.9117\n",
            "Epoch 603/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 246.8130 - val_loss: 233.6610\n",
            "Epoch 604/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 246.6009 - val_loss: 233.4363\n",
            "Epoch 605/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 246.3995 - val_loss: 233.1578\n",
            "Epoch 606/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 246.1964 - val_loss: 233.0158\n",
            "Epoch 607/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 245.9815 - val_loss: 232.8510\n",
            "Epoch 608/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 245.7766 - val_loss: 232.5978\n",
            "Epoch 609/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 245.5564 - val_loss: 232.2838\n",
            "Epoch 610/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 245.3662 - val_loss: 232.0370\n",
            "Epoch 611/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 245.1708 - val_loss: 231.7936\n",
            "Epoch 612/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 244.9644 - val_loss: 231.6674\n",
            "Epoch 613/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 244.7721 - val_loss: 231.8426\n",
            "Epoch 614/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 244.5259 - val_loss: 231.8012\n",
            "Epoch 615/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 244.3133 - val_loss: 231.7953\n",
            "Epoch 616/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 244.0986 - val_loss: 231.8293\n",
            "Epoch 617/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 243.9105 - val_loss: 231.8157\n",
            "Epoch 618/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 243.7037 - val_loss: 231.6985\n",
            "Epoch 619/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 243.4912 - val_loss: 231.3853\n",
            "Epoch 620/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 243.2835 - val_loss: 231.1159\n",
            "Epoch 621/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 243.0548 - val_loss: 231.0829\n",
            "Epoch 622/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 242.8454 - val_loss: 230.9468\n",
            "Epoch 623/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 242.6093 - val_loss: 230.6270\n",
            "Epoch 624/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 242.3875 - val_loss: 230.2612\n",
            "Epoch 625/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 242.1371 - val_loss: 229.9731\n",
            "Epoch 626/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 241.9104 - val_loss: 229.6344\n",
            "Epoch 627/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 241.7087 - val_loss: 229.3611\n",
            "Epoch 628/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 241.5030 - val_loss: 229.1351\n",
            "Epoch 629/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 241.3262 - val_loss: 228.8570\n",
            "Epoch 630/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 241.1118 - val_loss: 228.8029\n",
            "Epoch 631/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 240.9096 - val_loss: 228.4641\n",
            "Epoch 632/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 240.6573 - val_loss: 228.2365\n",
            "Epoch 633/900\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 240.4482 - val_loss: 228.0367\n",
            "Epoch 634/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 240.2375 - val_loss: 227.9698\n",
            "Epoch 635/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 240.0346 - val_loss: 227.9324\n",
            "Epoch 636/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 239.8150 - val_loss: 227.7497\n",
            "Epoch 637/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 239.6071 - val_loss: 227.5696\n",
            "Epoch 638/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 239.4067 - val_loss: 227.3725\n",
            "Epoch 639/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 239.2375 - val_loss: 227.1409\n",
            "Epoch 640/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 238.9749 - val_loss: 227.1413\n",
            "Epoch 641/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 238.7870 - val_loss: 227.2167\n",
            "Epoch 642/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 238.5743 - val_loss: 227.3085\n",
            "Epoch 643/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 238.4530 - val_loss: 227.4329\n",
            "Epoch 644/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 238.2445 - val_loss: 227.3029\n",
            "Epoch 645/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 238.0166 - val_loss: 226.8362\n",
            "Epoch 646/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 237.8082 - val_loss: 226.4060\n",
            "Epoch 647/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 237.5917 - val_loss: 226.1273\n",
            "Epoch 648/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 237.3786 - val_loss: 226.0226\n",
            "Epoch 649/900\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 237.1790 - val_loss: 225.8678\n",
            "Epoch 650/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 236.9846 - val_loss: 225.6788\n",
            "Epoch 651/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 236.7820 - val_loss: 225.5761\n",
            "Epoch 652/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 236.5949 - val_loss: 225.4229\n",
            "Epoch 653/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 236.3906 - val_loss: 225.1691\n",
            "Epoch 654/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 236.1818 - val_loss: 224.8837\n",
            "Epoch 655/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 235.9744 - val_loss: 224.6889\n",
            "Epoch 656/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 235.7783 - val_loss: 224.3805\n",
            "Epoch 657/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 235.5808 - val_loss: 224.1237\n",
            "Epoch 658/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 235.4111 - val_loss: 224.0132\n",
            "Epoch 659/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 235.2165 - val_loss: 223.8089\n",
            "Epoch 660/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 235.0287 - val_loss: 223.6142\n",
            "Epoch 661/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 234.8433 - val_loss: 223.3332\n",
            "Epoch 662/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 234.6700 - val_loss: 223.1345\n",
            "Epoch 663/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 234.4899 - val_loss: 223.0153\n",
            "Epoch 664/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 234.3126 - val_loss: 222.9194\n",
            "Epoch 665/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 234.1338 - val_loss: 222.9425\n",
            "Epoch 666/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 233.9564 - val_loss: 222.8380\n",
            "Epoch 667/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 233.7786 - val_loss: 222.5754\n",
            "Epoch 668/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 233.5827 - val_loss: 222.4274\n",
            "Epoch 669/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 233.3922 - val_loss: 222.3504\n",
            "Epoch 670/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 233.2170 - val_loss: 222.2705\n",
            "Epoch 671/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 233.0401 - val_loss: 222.1995\n",
            "Epoch 672/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 232.8708 - val_loss: 222.0846\n",
            "Epoch 673/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 232.7034 - val_loss: 221.9292\n",
            "Epoch 674/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 232.5277 - val_loss: 221.8131\n",
            "Epoch 675/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 232.3539 - val_loss: 221.6257\n",
            "Epoch 676/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 232.1782 - val_loss: 221.3769\n",
            "Epoch 677/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 232.0223 - val_loss: 221.1364\n",
            "Epoch 678/900\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 231.8399 - val_loss: 220.9514\n",
            "Epoch 679/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 231.6856 - val_loss: 220.7510\n",
            "Epoch 680/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 231.4998 - val_loss: 220.6861\n",
            "Epoch 681/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 231.3535 - val_loss: 220.6707\n",
            "Epoch 682/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 231.1517 - val_loss: 220.5185\n",
            "Epoch 683/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 230.9871 - val_loss: 220.4761\n",
            "Epoch 684/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 230.8488 - val_loss: 220.5327\n",
            "Epoch 685/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 230.7013 - val_loss: 220.4579\n",
            "Epoch 686/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 230.5280 - val_loss: 220.2650\n",
            "Epoch 687/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 230.3651 - val_loss: 220.1790\n",
            "Epoch 688/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 230.1946 - val_loss: 219.8699\n",
            "Epoch 689/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 230.0285 - val_loss: 219.6002\n",
            "Epoch 690/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 229.8853 - val_loss: 219.2918\n",
            "Epoch 691/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 229.7451 - val_loss: 218.9424\n",
            "Epoch 692/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 229.5942 - val_loss: 218.7398\n",
            "Epoch 693/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 229.4085 - val_loss: 218.7696\n",
            "Epoch 694/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 229.2209 - val_loss: 218.7589\n",
            "Epoch 695/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 229.0770 - val_loss: 218.5464\n",
            "Epoch 696/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 228.8955 - val_loss: 218.3924\n",
            "Epoch 697/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 228.7356 - val_loss: 218.3046\n",
            "Epoch 698/900\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 228.5577 - val_loss: 218.1781\n",
            "Epoch 699/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 228.3965 - val_loss: 218.0416\n",
            "Epoch 700/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 228.2525 - val_loss: 217.9082\n",
            "Epoch 701/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 228.0729 - val_loss: 217.9234\n",
            "Epoch 702/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 227.8830 - val_loss: 218.0102\n",
            "Epoch 703/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 227.7998 - val_loss: 218.1240\n",
            "Epoch 704/900\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 227.6258 - val_loss: 217.7873\n",
            "Epoch 705/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 227.4993 - val_loss: 217.4374\n",
            "Epoch 706/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 227.2692 - val_loss: 217.4095\n",
            "Epoch 707/900\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 227.1310 - val_loss: 217.3377\n",
            "Epoch 708/900\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 226.9908 - val_loss: 217.2088\n",
            "Epoch 709/900\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 226.8483 - val_loss: 217.0314\n",
            "Epoch 710/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 226.7021 - val_loss: 216.8649\n",
            "Epoch 711/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 226.5647 - val_loss: 216.7486\n",
            "Epoch 712/900\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 226.4099 - val_loss: 216.7651\n",
            "Epoch 713/900\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 226.2726 - val_loss: 216.7994\n",
            "Epoch 714/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 226.1324 - val_loss: 216.6175\n",
            "Epoch 715/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 225.9739 - val_loss: 216.4719\n",
            "Epoch 716/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 225.8245 - val_loss: 216.2939\n",
            "Epoch 717/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 225.6906 - val_loss: 216.2137\n",
            "Epoch 718/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 225.5179 - val_loss: 215.9679\n",
            "Epoch 719/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 225.3736 - val_loss: 215.8795\n",
            "Epoch 720/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 225.2477 - val_loss: 215.9420\n",
            "Epoch 721/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 225.1340 - val_loss: 215.8898\n",
            "Epoch 722/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 225.0209 - val_loss: 215.8027\n",
            "Epoch 723/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 224.9032 - val_loss: 215.6744\n",
            "Epoch 724/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 224.7489 - val_loss: 215.5571\n",
            "Epoch 725/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 224.6251 - val_loss: 215.3291\n",
            "Epoch 726/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 224.4788 - val_loss: 214.9900\n",
            "Epoch 727/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 224.3197 - val_loss: 214.6884\n",
            "Epoch 728/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 224.1864 - val_loss: 214.5805\n",
            "Epoch 729/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 224.0377 - val_loss: 214.2831\n",
            "Epoch 730/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 223.9495 - val_loss: 214.0545\n",
            "Epoch 731/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 223.8465 - val_loss: 213.9954\n",
            "Epoch 732/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 223.7232 - val_loss: 213.8446\n",
            "Epoch 733/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 223.5955 - val_loss: 213.8207\n",
            "Epoch 734/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 223.4568 - val_loss: 213.6774\n",
            "Epoch 735/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 223.3306 - val_loss: 213.6501\n",
            "Epoch 736/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 223.1955 - val_loss: 213.6377\n",
            "Epoch 737/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 223.0647 - val_loss: 213.6149\n",
            "Epoch 738/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 222.9541 - val_loss: 213.5356\n",
            "Epoch 739/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 222.8857 - val_loss: 213.5396\n",
            "Epoch 740/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 222.6943 - val_loss: 213.2631\n",
            "Epoch 741/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 222.5714 - val_loss: 213.0042\n",
            "Epoch 742/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 222.5071 - val_loss: 212.7654\n",
            "Epoch 743/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 222.3928 - val_loss: 212.6579\n",
            "Epoch 744/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 222.2691 - val_loss: 212.6124\n",
            "Epoch 745/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 222.1523 - val_loss: 212.4702\n",
            "Epoch 746/900\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 222.0280 - val_loss: 212.4328\n",
            "Epoch 747/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 221.8904 - val_loss: 212.4791\n",
            "Epoch 748/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 221.7483 - val_loss: 212.5473\n",
            "Epoch 749/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 221.6324 - val_loss: 212.5187\n",
            "Epoch 750/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 221.5208 - val_loss: 212.4483\n",
            "Epoch 751/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 221.4074 - val_loss: 212.3926\n",
            "Epoch 752/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 221.3061 - val_loss: 212.3340\n",
            "Epoch 753/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 221.1905 - val_loss: 212.2324\n",
            "Epoch 754/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 221.0486 - val_loss: 212.0181\n",
            "Epoch 755/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 220.9342 - val_loss: 211.8371\n",
            "Epoch 756/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 220.8223 - val_loss: 211.7668\n",
            "Epoch 757/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 220.7023 - val_loss: 211.7907\n",
            "Epoch 758/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 220.6308 - val_loss: 211.7924\n",
            "Epoch 759/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 220.4848 - val_loss: 211.5977\n",
            "Epoch 760/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 220.3771 - val_loss: 211.3593\n",
            "Epoch 761/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 220.2689 - val_loss: 211.0682\n",
            "Epoch 762/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 220.1632 - val_loss: 210.9448\n",
            "Epoch 763/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 220.0516 - val_loss: 210.9724\n",
            "Epoch 764/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 219.9360 - val_loss: 210.9298\n",
            "Epoch 765/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 219.8224 - val_loss: 210.9220\n",
            "Epoch 766/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 219.7112 - val_loss: 210.9532\n",
            "Epoch 767/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 219.6219 - val_loss: 210.8736\n",
            "Epoch 768/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 219.4980 - val_loss: 210.7132\n",
            "Epoch 769/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 219.3882 - val_loss: 210.6748\n",
            "Epoch 770/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 219.2923 - val_loss: 210.7595\n",
            "Epoch 771/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 219.1958 - val_loss: 210.6829\n",
            "Epoch 772/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 219.0635 - val_loss: 210.4807\n",
            "Epoch 773/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 218.9785 - val_loss: 210.2248\n",
            "Epoch 774/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 218.8422 - val_loss: 210.1444\n",
            "Epoch 775/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 218.7162 - val_loss: 210.1450\n",
            "Epoch 776/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 218.6067 - val_loss: 210.1860\n",
            "Epoch 777/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 218.5221 - val_loss: 210.0100\n",
            "Epoch 778/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 218.4213 - val_loss: 209.6707\n",
            "Epoch 779/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 218.2981 - val_loss: 209.4730\n",
            "Epoch 780/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 218.2144 - val_loss: 209.2764\n",
            "Epoch 781/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 218.1745 - val_loss: 209.1645\n",
            "Epoch 782/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 218.0762 - val_loss: 209.3876\n",
            "Epoch 783/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 217.9183 - val_loss: 209.3241\n",
            "Epoch 784/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 217.8054 - val_loss: 209.4645\n",
            "Epoch 785/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 217.7198 - val_loss: 209.5147\n",
            "Epoch 786/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 217.6315 - val_loss: 209.4798\n",
            "Epoch 787/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 217.5542 - val_loss: 209.2744\n",
            "Epoch 788/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 217.4394 - val_loss: 209.1286\n",
            "Epoch 789/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 217.3588 - val_loss: 209.0304\n",
            "Epoch 790/900\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 217.2375 - val_loss: 209.0763\n",
            "Epoch 791/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 217.1622 - val_loss: 209.1226\n",
            "Epoch 792/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 217.1104 - val_loss: 209.0912\n",
            "Epoch 793/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 216.9910 - val_loss: 208.8044\n",
            "Epoch 794/900\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 216.8690 - val_loss: 208.3765\n",
            "Epoch 795/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 216.8079 - val_loss: 208.1504\n",
            "Epoch 796/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 216.7129 - val_loss: 208.1526\n",
            "Epoch 797/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 216.6770 - val_loss: 208.3963\n",
            "Epoch 798/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 216.5387 - val_loss: 208.1997\n",
            "Epoch 799/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 216.4418 - val_loss: 208.0499\n",
            "Epoch 800/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 216.3574 - val_loss: 207.9739\n",
            "Epoch 801/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 216.2682 - val_loss: 208.0701\n",
            "Epoch 802/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 216.1729 - val_loss: 207.9124\n",
            "Epoch 803/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 216.0862 - val_loss: 207.7983\n",
            "Epoch 804/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 215.9847 - val_loss: 207.8214\n",
            "Epoch 805/900\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 215.9355 - val_loss: 207.5183\n",
            "Epoch 806/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 215.7823 - val_loss: 207.3921\n",
            "Epoch 807/900\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 215.7732 - val_loss: 207.0724\n",
            "Epoch 808/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 215.6371 - val_loss: 206.9685\n",
            "Epoch 809/900\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 215.5415 - val_loss: 206.9534\n",
            "Epoch 810/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 215.4490 - val_loss: 207.1170\n",
            "Epoch 811/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 215.3171 - val_loss: 207.1004\n",
            "Epoch 812/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 215.2385 - val_loss: 207.0053\n",
            "Epoch 813/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 215.1239 - val_loss: 206.7965\n",
            "Epoch 814/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 215.0550 - val_loss: 206.7311\n",
            "Epoch 815/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 214.9674 - val_loss: 206.6942\n",
            "Epoch 816/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 214.8879 - val_loss: 206.6134\n",
            "Epoch 817/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 214.8296 - val_loss: 206.4547\n",
            "Epoch 818/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 214.7181 - val_loss: 206.5473\n",
            "Epoch 819/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 214.6289 - val_loss: 206.5054\n",
            "Epoch 820/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 214.5546 - val_loss: 206.4223\n",
            "Epoch 821/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 214.4776 - val_loss: 206.5722\n",
            "Epoch 822/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 214.3798 - val_loss: 206.6519\n",
            "Epoch 823/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 214.3276 - val_loss: 206.7361\n",
            "Epoch 824/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 214.2540 - val_loss: 206.6512\n",
            "Epoch 825/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 214.1669 - val_loss: 206.5202\n",
            "Epoch 826/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 214.0622 - val_loss: 206.4784\n",
            "Epoch 827/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 213.9684 - val_loss: 206.3578\n",
            "Epoch 828/900\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 213.8782 - val_loss: 206.2297\n",
            "Epoch 829/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 213.7850 - val_loss: 206.1927\n",
            "Epoch 830/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 213.6916 - val_loss: 206.1849\n",
            "Epoch 831/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 213.6081 - val_loss: 206.1566\n",
            "Epoch 832/900\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 213.5533 - val_loss: 206.1468\n",
            "Epoch 833/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 213.4449 - val_loss: 206.0101\n",
            "Epoch 834/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 213.3678 - val_loss: 205.8403\n",
            "Epoch 835/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 213.2967 - val_loss: 205.8509\n",
            "Epoch 836/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 213.2107 - val_loss: 205.5748\n",
            "Epoch 837/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 213.0836 - val_loss: 205.4317\n",
            "Epoch 838/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 212.9717 - val_loss: 205.4325\n",
            "Epoch 839/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 212.8825 - val_loss: 205.4195\n",
            "Epoch 840/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 212.8105 - val_loss: 205.5071\n",
            "Epoch 841/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 212.7458 - val_loss: 205.5718\n",
            "Epoch 842/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 212.6737 - val_loss: 205.6194\n",
            "Epoch 843/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 212.5882 - val_loss: 205.3496\n",
            "Epoch 844/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 212.4458 - val_loss: 205.1156\n",
            "Epoch 845/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 212.3652 - val_loss: 204.8620\n",
            "Epoch 846/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 212.3440 - val_loss: 204.7141\n",
            "Epoch 847/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 212.2371 - val_loss: 204.6668\n",
            "Epoch 848/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 212.1345 - val_loss: 204.6928\n",
            "Epoch 849/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 212.0062 - val_loss: 204.7683\n",
            "Epoch 850/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 211.9238 - val_loss: 204.8277\n",
            "Epoch 851/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 211.8971 - val_loss: 204.8496\n",
            "Epoch 852/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 211.7823 - val_loss: 204.6653\n",
            "Epoch 853/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 211.6800 - val_loss: 204.5447\n",
            "Epoch 854/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 211.5923 - val_loss: 204.4256\n",
            "Epoch 855/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 211.5544 - val_loss: 204.3373\n",
            "Epoch 856/900\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 211.4460 - val_loss: 204.4162\n",
            "Epoch 857/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 211.4087 - val_loss: 204.6376\n",
            "Epoch 858/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 211.3093 - val_loss: 204.6279\n",
            "Epoch 859/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 211.2381 - val_loss: 204.5267\n",
            "Epoch 860/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 211.1830 - val_loss: 204.2745\n",
            "Epoch 861/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 211.0799 - val_loss: 204.1750\n",
            "Epoch 862/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 210.9712 - val_loss: 204.1494\n",
            "Epoch 863/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 210.8993 - val_loss: 204.0889\n",
            "Epoch 864/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 210.8175 - val_loss: 204.0301\n",
            "Epoch 865/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 210.7461 - val_loss: 203.9034\n",
            "Epoch 866/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 210.6994 - val_loss: 203.8542\n",
            "Epoch 867/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 210.6283 - val_loss: 203.8181\n",
            "Epoch 868/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 210.5509 - val_loss: 203.8299\n",
            "Epoch 869/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 210.4624 - val_loss: 203.8068\n",
            "Epoch 870/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 210.3868 - val_loss: 203.7753\n",
            "Epoch 871/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 210.3077 - val_loss: 203.8286\n",
            "Epoch 872/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 210.2480 - val_loss: 203.7329\n",
            "Epoch 873/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 210.1576 - val_loss: 203.6415\n",
            "Epoch 874/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 210.0961 - val_loss: 203.6602\n",
            "Epoch 875/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 210.0226 - val_loss: 203.6640\n",
            "Epoch 876/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 209.9650 - val_loss: 203.6444\n",
            "Epoch 877/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 209.8930 - val_loss: 203.5891\n",
            "Epoch 878/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 209.8206 - val_loss: 203.4557\n",
            "Epoch 879/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 209.7820 - val_loss: 203.3395\n",
            "Epoch 880/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 209.7341 - val_loss: 203.3074\n",
            "Epoch 881/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 209.6628 - val_loss: 203.2613\n",
            "Epoch 882/900\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 209.5883 - val_loss: 203.2814\n",
            "Epoch 883/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 209.4952 - val_loss: 203.3019\n",
            "Epoch 884/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 209.4227 - val_loss: 203.3431\n",
            "Epoch 885/900\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 209.3832 - val_loss: 203.2644\n",
            "Epoch 886/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 209.3060 - val_loss: 203.2299\n",
            "Epoch 887/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 209.2345 - val_loss: 203.1569\n",
            "Epoch 888/900\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 209.1591 - val_loss: 203.0629\n",
            "Epoch 889/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 209.1352 - val_loss: 202.9889\n",
            "Epoch 890/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 209.0685 - val_loss: 202.9844\n",
            "Epoch 891/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 208.9912 - val_loss: 202.9635\n",
            "Epoch 892/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 208.8987 - val_loss: 203.0139\n",
            "Epoch 893/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 208.8671 - val_loss: 203.1157\n",
            "Epoch 894/900\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 208.8344 - val_loss: 203.0951\n",
            "Epoch 895/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 208.7731 - val_loss: 203.0464\n",
            "Epoch 896/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 208.6973 - val_loss: 202.9325\n",
            "Epoch 897/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 208.6501 - val_loss: 202.7815\n",
            "Epoch 898/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 208.5672 - val_loss: 202.7377\n",
            "Epoch 899/900\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 208.5155 - val_loss: 202.7187\n",
            "Epoch 900/900\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 208.4407 - val_loss: 202.6493\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc7dd70f908>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prQbxU-5u1ei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get a history of the losses\n",
        "# model.history.history\n",
        "\n",
        "# put the loss history in a dataframe for better visualization\n",
        "losses = pd.DataFrame(model.history.history, columns = [\"training_loss\", \"val_loss\"])"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zt260iXIvdR4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "224250b1-653b-47c5-ae54-55bb532ed358"
      },
      "source": [
        "losses.plot()"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fc7dc73f0f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXwV5d338c8vCwRClABhS4CADTsaICIIWFdEqqJWRGwrrtxVW623j631bqu1+tSqtS33rXBja10eURG1oMUFFaVYUAKGHdlkSQgQFtk0kOV6/pgJHCBAlpPMyTnf9+s1r5lzzTVzfmdy8ps51yyXOecQEZHYEBd0ACIiUn+U9EVEYoiSvohIDFHSFxGJIUr6IiIxJCHoAE6mVatWLjMzM+gwREQajAULFmx3zqVVNi/ik35mZia5ublBhyEi0mCY2YbjzVPzjohIDFHSFxGJIUr6IiIxJOLb9EUk8pSUlJCfn09xcXHQocS0pKQkMjIySExMrPIySvoiUm35+fmkpKSQmZmJmQUdTkxyzrFjxw7y8/Pp3LlzlZdT846IVFtxcTEtW7ZUwg+QmdGyZctq/9pS0heRGlHCD15N/gbR27zzyWNgcdCoGTRqCo2SITEZkk6FlLbekNgk6ChFROpV9Cb9OX+Gkv0nrpPUHFIzoXUPb+g4CNr3g/jo3SwiEtuiN7v912YoPQgH98HB/VDyjTf97S7YuxX2FnrDjrWwdhYsetlbrlEKdD4Hel0J3S6Bxs2C/Rwicoyvv/6ayZMnc/vtt1druREjRjB58mSaN29+3Dq/+c1vOOecc7jwwgtrG+YhzZo1Y9++fWFbX21Eb9IHSGgECS2gaYuT192/Hdb/C9Z9Aqvfhy//CYlNoc8oGHg7tO5e9/GKSJV8/fXXPP3008ck/dLSUhISjp/WZsyYcdJ1P/TQQ7WOL5JFd9KvjuRW3tF9ryuhvBw2zfOO/he/Cgufh66XwIUPKvmLHOW3by1j+eY9YV1nz/an8MBlvY47/7777mPt2rVkZ2eTmJhIUlISqamprFy5klWrVnHFFVewadMmiouLueuuuxg3bhxw+Fle+/bt45JLLmHIkCH8+9//Jj09nWnTptGkSRNuuOEGLr30Uq6++moyMzMZO3Ysb731FiUlJbz22mt0796doqIirrvuOjZv3sygQYOYOXMmCxYsoFWrVif8XM45fv7zn/POO+9gZvzqV79i9OjRFBYWMnr0aPbs2UNpaSkTJkzg7LPP5uabbyY3Nxcz46abbuLuu++u9bbV1TuViYuDTmfD5f8Ndy+Hc++HDZ/ChEHw1s/gm51BRygS0x599FFOO+008vLyePzxx1m4cCF/+ctfWLVqFQDPPvssCxYsIDc3l/Hjx7Njx45j1rF69WruuOMOli1bRvPmzXn99dcrfa9WrVqxcOFCbrvtNp544gkAfvvb33L++eezbNkyrr76ajZu3FiluN944w3y8vJYtGgRH3zwAffeey+FhYVMnjyZiy+++NC87Oxs8vLyKCgoYOnSpSxZsoQbb7yxhlvrSDrSP5nklnDuL+DMW+CTP8D8v8KXM+DSP0P3EUFHJxK4Ex2R15cBAwYccYPS+PHjefPNNwHYtGkTq1evpmXLlkcs07lzZ7KzswHo378/69evr3TdV1111aE6b7zxBgBz5sw5tP7hw4eTmppapTjnzJnDmDFjiI+Pp02bNnz3u99l/vz5nHnmmdx0002UlJRwxRVXkJ2dTZcuXVi3bh0//elP+d73vsewYcOqvkFOQEf6VZXcEkY8Brd+BMlp8MoYeOcXUFYSdGQiMS85OfnQ9Mcff8wHH3zA3LlzWbRoEX379q30BqbGjRsfmo6Pj6e0tLTSdVfUO1Gd2jrnnHOYPXs26enp3HDDDbzwwgukpqayaNEizj33XCZOnMgtt9wSlvdS0q+u9tlw6yzv5O5nE+H5y7yTwCJSb1JSUti7d2+l83bv3k1qaipNmzZl5cqVzJs3L+zvP3jwYKZMmQLA+++/z65du6q03NChQ3n11VcpKyujqKiI2bNnM2DAADZs2ECbNm249dZbueWWW1i4cCHbt2+nvLyc73//+zz88MMsXLgwLLGreacmEhrB8N9Den+Ydgf8fQRcPw1OaRd0ZCIxoWXLlgwePJjevXvTpEkT2rRpc2je8OHDmThxIj169KBbt24MHDgw7O//wAMPMGbMGF588UUGDRpE27ZtSUlJOelyV155JXPnzuWMM87AzHjsscdo27Ytzz//PI8//jiJiYk0a9aMF154gYKCAm688UbKy8sB+P3vfx+W2M05F5YV1ZWcnBwX0T1nrZ8Dk0dDs9Zw03veWCTKrVixgh49egQdRmAOHDhAfHw8CQkJzJ07l9tuu428vLxAYqnsb2FmC5xzOZXVP2nzjpl1MLNZZrbczJaZ2V1+eQszm2lmq/1xql9uZjbezNaY2WIz6xeyrrF+/dVmNrZWnzRSZA6BH70JewrhpVFwIDJuwBCRurNx40bOPPNMzjjjDO68806eeeaZoEOqsqo075QC9zjnFppZCrDAzGYCNwAfOuceNbP7gPuAXwCXAFn+cBYwATjLzFoADwA5gPPXM905V7XGsEjWYQCMeg5euQ6mXA8/eA3i4oOOSkTqSFZWFl988cURZTt27OCCCy44pu6HH354zJVDQTpp0nfOFQKF/vReM1sBpAMjgXP9as8DH+Ml/ZHAC85rN5pnZs3NrJ1fd6ZzbieAv+MYDrwcxs8TnG7D4dIn4a27YNYjcMFvgo5IROpRy5YtA2viqY5qXb1jZplAX+AzoI2/QwDYAlScSUkHNoUslu+XHa+8svcZZ2a5ZpZbVFRUnRCD1f8G6DcW/vVHWHny271FROpblZO+mTUDXgd+5pw74p5r/6g+bGeEnXOTnHM5zrmctLS0cK22flzyGLTLhjd/DLvzg45GROQIVUr6ZpaIl/Bfcs694Rdv9Ztt8Mfb/PICoEPI4hl+2fHKo0tiEoz6O5SXwrSfQIRfHSUisaUqV+8Y8DdghXPuyZBZ04GKK3DGAtNCyq/3r+IZCOz2m4HeA4aZWap/pc8wvyz6tOgCw34H62ZB7rNBRyMickhVjvQHAz8CzjezPH8YATwKXGRmq4EL/dcAM4B1wBrgGeB2AP8E7u+A+f7wUMVJ3aiUcxN0OQ/e/7WaeUQC1qzZ8fvFWL9+Pb17967HaIJVlat35gDH64jxmOuT/Pb9O46zrmeB2Dj0NYPLx8P/DID37odrXgg6IhERPYahTjXvCOfcAx89DGs/gtPODzoikfB75z7YsiS862zbBy559Liz77vvPjp06MAdd3jHlw8++CAJCQnMmjWLXbt2UVJSwsMPP8zIkSOr9bbFxcXcdttt5ObmkpCQwJNPPsl5553HsmXLuPHGGzl48CDl5eW8/vrrtG/fnmuuuYb8/HzKysr49a9/zejRo2v1seuDHrhW186+02vjn3Gv132jiNTa6NGjDz3wDGDKlCmMHTuWN998k4ULFzJr1izuueceqvuYmaeeegozY8mSJbz88suMHTuW4uJiJk6cyF133UVeXh65ublkZGTw7rvv0r59exYtWsTSpUsZPnx4uD9mndCRfl1LaAzD/wCTR3k9cA24NeiIRMLrBEfkdaVv375s27aNzZs3U1RURGpqKm3btuXuu+9m9uzZxMXFUVBQwNatW2nbtm2V1ztnzhx++tOfAtC9e3c6derEqlWrGDRoEI888gj5+flcddVVZGVl0adPH+655x5+8YtfcOmllzJ06NC6+rhhpSP9+pB1EWQO9TphOVD542BFpHpGjRrF1KlTefXVVxk9ejQvvfQSRUVFLFiwgLy8PNq0aVPpc/Rr4rrrrmP69Ok0adKEESNG8NFHH9G1a1cWLlxInz59+NWvftVg+tZV0q8PZnDhb2F/Ecx9KuhoRKLC6NGjeeWVV5g6dSqjRo1i9+7dtG7dmsTERGbNmsWGDRuqvc6hQ4fy0ksvAbBq1So2btxIt27dWLduHV26dOHOO+9k5MiRLF68mM2bN9O0aVN++MMfcu+994btefd1Tc079SWjP/QcCZ+O97peTD5xB8oicmK9evVi7969pKen065dO37wgx9w2WWX0adPH3JycujevXu113n77bdz22230adPHxISEnjuuedo3LgxU6ZM4cUXXyQxMZG2bdty//33M3/+fO69917i4uJITExkwoQJdfApw0/P069PRavgqQEw5G648IGgoxGpsVh/nn4kCfvz9CWM0rpCryvg82fg24b/RGkRaXiU9Ovb0P8DB/d6iV9E6s2SJUvIzs4+YjjrrLOCDqveqU2/vrXtDd1GwLynYeBt0Pjk/WqKRCLnHN6juRqGPn36NIjn3VdHTZrndaQfhKH/x2veyf170JGI1EhSUhI7duyoUdKR8HDOsWPHDpKSkqq1nI70g5DRHzoNgc8nwcDbIV5/BmlYMjIyyM/Pp0F1chSFkpKSyMjIqNYyyjZBGfhjePWH8OUM6Hl50NGIVEtiYiKdO3cOOgypATXvBKXbCO+BbJ9NDDoSEYkhSvpBiYuHAeNgw6dQuDjoaEQkRijpB6nvjyAxWUf7IlJvlPSD1KQ5nDEalr6um7VEpF5UpY/cZ81sm5ktDSl7NaTrxPVmlueXZ5rZtyHzJoYs09/MlpjZGjMbbw3pAt+61P8GKC2GJVODjkREYkBVjvSfA47oHcA5N9o5l+2cywZeB94Imb22Yp5z7sch5ROAW4Esf2gYPQ7UtXZneMOC50HXPItIHTtp0nfOzQYq7cDcP1q/Bnj5ROsws3bAKc65eX4fui8AV1Q/3CjV73rYugQKo+tuQRGJPLVt0x8KbHXOrQ4p62xmX5jZJ2ZW0ZVMOpAfUiffL6uUmY0zs1wzy42Jmz96Xw0JTWChOk8XkbpV26Q/hiOP8guBjs65vsB/ApPN7JTqrtQ5N8k5l+Ocy0lLS6tliA1Ak+bes/aXTIWD+4OORkSiWI2TvpklAFcBr1aUOecOOOd2+NMLgLVAV6AACL1XOMMvkwr9fgQH9sDKfwYdiYhEsdoc6V8IrHTOHWq2MbM0M4v3p7vgnbBd55wrBPaY2UD/PMD1wLRavHf06Xg2nJIBS14LOhIRiWJVuWTzZWAu0M3M8s3sZn/WtRx7AvccYLF/CedU4MfOuYqTwLcDfwXW4P0CeCcM8UePuDjoczWs+RD2bw86GhGJUuouMZJsWQoTB8OIJ2DArUFHIyINlLpLbCja9obWPdXEIyJ1Rkk/0vQZBZs+g13rg45ERKKQkn6k6XO1N9bRvojUASX9SNO8I2QMgOW6uElEwk9JPxL1vBy2LIGdXwUdiYhEGSX9SNTjMm+84q1g4xCRqKOkH4lSM6Ht6bBietCRiEiUUdKPVD0vh/z5sGdz0JGISBRR0o9UPUZ64xVvBxuHiEQVJf1IldYVWnVTE4+IhJWSfiTreTls+FTP4hGRsFHSj2Q9LgNXDqveDToSEYkSSvqRrO3pkNIeVr0XdCQiEiWU9COZGXQdBmtnQenBoKMRkSigpB/psi6Gg3th47+DjkREooCSfqTr8l2Ibwyr3g86EhGJAkr6ka5RMnQeqpO5IhIWVeku8Vkz22ZmS0PKHjSzAjPL84cRIfN+aWZrzOxLM7s4pHy4X7bGzO4L/0eJYlkXw861sGNt0JGISANXlSP954DhlZT/yTmX7Q8zAMysJ17fub38ZZ42s3i/s/SngEuAnsAYv65URddh3lhX8YhILZ006TvnZgM7T1bPNxJ4xTl3wDn3FV4n6AP8YY1zbp1z7iDwil9XqiI1E9K6w2q164tI7dSmTf8nZrbYb/5J9cvSgU0hdfL9suOVV8rMxplZrpnlFhUV1SLEKHLa+bBxLpR8G3QkItKA1TTpTwBOA7KBQuCPYYsIcM5Ncs7lOOdy0tLSwrnqhqvLeVBa7CV+EZEaqlHSd85tdc6VOefKgWfwmm8ACoAOIVUz/LLjlUtVZQ6GuETvRi0RkRqqUdI3s3YhL68EKq7smQ5ca2aNzawzkAV8DswHssyss5k1wjvZq8dHVkejZOg4UElfRGol4WQVzOxl4FyglZnlAw8A55pZNuCA9cB/ADjnlpnZFGA5UArc4Zwr89fzE+A9IB541jm3LOyfJtqddh58+BDs2wbNWgcdjYg0QOacCzqGE8rJyXG5ublBhxEZChbCM+fBVc/A6dcEHY2IRCgzW+Ccy6lsnu7IbUjanQFNUtXEIyI1pqTfkMTFQ5dzYe1HEOG/0EQkMinpNzRdzoV9W2D76qAjEZEGSEm/ockc6o3X/yvYOESkQVLSb2hadPF601o/J+hIRKQBUtJvaMwgc4iX9NWuLyLVpKTfEGUOgf3b1K4vItWmpN8QZQ7xxmrXF5FqUtJviNSuLyI1pKTfEKldX0RqSEm/oVK7vojUgJJ+Q6V2fRGpASX9hkrt+iJSA0r6DZUZdDrb60lL7foiUkVK+g1Zx4GwtxC+3hh0JCLSQCjpN2QdzvLGmz4LNg4RaTBOmvTN7Fkz22ZmS0PKHjezlWa22MzeNLPmfnmmmX1rZnn+MDFkmf5mtsTM1pjZeDOzuvlIMaRNL2iUAhvnBR2JiDQQVTnSfw4YflTZTKC3c+50YBXwy5B5a51z2f7w45DyCcCteP3mZlWyTqmuuHjIyNGRvohU2UmTvnNuNrDzqLL3nXOl/st5QMaJ1uF3pH6Kc26e8/pnfAG4omYhyxE6DoSty6B4d9CRiEgDEI42/ZuAd0JedzazL8zsEzPzH/5OOpAfUiffL6uUmY0zs1wzyy0qKgpDiFGsw1mAg/z5QUciIg1ArZK+mf0XUAq85BcVAh2dc32B/wQmm9kp1V2vc26Scy7HOZeTlpZWmxCjX0YOWBxsVBOPiJxcQk0XNLMbgEuBC/wmG5xzB4AD/vQCM1sLdAUKOLIJKMMvk9pqnAJtesMmncwVkZOr0ZG+mQ0Hfg5c7pz7JqQ8zczi/ekueCds1znnCoE9ZjbQv2rnemBaraMXT4ezIH8BlJWevK6IxLSqXLL5MjAX6GZm+WZ2M/A/QAow86hLM88BFptZHjAV+LFzruIk8O3AX4E1wFqOPA8gtdFxIJTsh61Lgo5ERCLcSZt3nHNjKin+23Hqvg68fpx5uUDvakUnVVNxk9bGz6B932BjEZGIpjtyo0HzDpDSDgpyg45ERCKckn60SO8PBQuCjkJEIpySfrRI7w8718E3O09eV0RilpJ+tMjI8cY62heRE1DSjxbt+wKmpC8iJ6SkHy0ap0Bad8jXyVwROT4l/WiS4Z/MVU9aInIcSvrRJD0Hvt0Ju74KOhIRiVBK+tEkvb83zle7vohUTkk/mrTuCYlNdZOWiByXkn40iU+Adtm6gkdEjktJP9qk94PCxVB6MOhIRCQCKelHm4wcKDugJ26KSKWU9KNNun9nrk7mikgllPSjzakZkNwaNi8MOhIRiUBK+tHGzGvXL1DSF5FjKelHo/b9YPsqOLA36EhEJMJUKemb2bNmts3MloaUtTCzmWa22h+n+uVmZuPNbI2ZLTazfiHLjPXrrzazseH/OAJ4R/o42JwXdCQiEmGqeqT/HDD8qLL7gA+dc1nAh/5rgEvwOkTPAsYBE8DbSQAPAGcBA4AHKnYUEmbt/f2srtcXkaNUKek752YDR/fOMRJ43p9+HrgipPwF55kHNDezdsDFwEzn3E7n3C5gJsfuSCQckltC8446mSsix6hNm34b51yhP70FaONPpwObQurl+2XHKz+GmY0zs1wzyy0qKqpFiDGsfT8o+CLoKEQkwoTlRK5zzgFhe56vc26Scy7HOZeTlpYWrtXGlvT+sHsj7N8edCQiEkFqk/S3+s02+ONtfnkB0CGkXoZfdrxyqQvpFe36auIRkcNqk/SnAxVX4IwFpoWUX+9fxTMQ2O03A70HDDOzVP8E7jC/TOpCuzMAU7u+iBwhoSqVzOxl4FyglZnl412F8ygwxcxuBjYA1/jVZwAjgDXAN8CNAM65nWb2O2C+X+8h59zRJ4clXBqnQFo3HemLyBGqlPSdc2OOM+uCSuo64I7jrOdZ4NkqRye1074frJnpdZ9oFnQ0IhIBdEduNEvvB/uLYHd+0JGISIRQ0o9muklLRI6ipB/N2vaGuESdzBWRQ5T0o1lCY2jTSydzReQQJf1ol94PChdBeXnQkYhIBFDSj3bp/eHAHtixJuhIRCQCKOlHu4qTuWrXFxGU9KNfWjdITFa7vogASvrRLy7eeySDjvRFBCX92JDeDwoXQ1lJ0JGISMCU9GNB+75QdgC2Lgs6EhEJmJJ+LEjXyVwR8Sjpx4LUztAkVSdzRURJPyaYeU08m9V9okisU9KPFRlnwrblULw76EhEJEBK+rGi09ngymHjZ0FHIiIBUtKPFRkDvCdubpgTdCQiEqAaJ30z62ZmeSHDHjP7mZk9aGYFIeUjQpb5pZmtMbMvzezi8HwEqZJGTb2reNZ/GnQkIhKgGid959yXzrls51w20B+vP9w3/dl/qpjnnJsBYGY9gWuBXsBw4Gkzi69d+FItmUO8k7kH9gUdiYgEJFzNOxcAa51zG05QZyTwinPugHPuK7yO0weE6f2lKjoNBlcGG+cFHYmIBCRcSf9a4OWQ1z8xs8Vm9qyZpfpl6cCmkDr5ftkxzGycmeWaWW5RUVGYQhQ6DoKEJFjzQdCRiEhAap30zawRcDnwml80ATgNyAYKgT9Wd53OuUnOuRznXE5aWlptQ5QKjZp6TTxrZgYdiYgEJBxH+pcAC51zWwGcc1udc2XOuXLgGQ434RQAHUKWy/DLpD5lDfM6VNm5LuhIRCQA4Uj6Ywhp2jGzdiHzrgSW+tPTgWvNrLGZdQaygM/D8P5SHd+50BuvVhOPSCyqVdI3s2TgIuCNkOLHzGyJmS0GzgPuBnDOLQOmAMuBd4E7nHNltXl/qYGWp0HL78CX/ww6EhEJQEJtFnbO7QdaHlX2oxPUfwR4pDbvKWHQcyTM+TPs3w7JrYKORkTqke7IjUW9rvQu3VzxVtCRiEg9U9KPRW16Q8ssWPRK0JGISD1T0o9FZtB/LGyaB1uWnry+iEQNJf1Ylf0D70at3L8FHYmI1CMl/VjVtAX0/j4sehWK9wQdjYjUEyX9WHbmzVCyH774f0FHIiL1REk/lqX3h05D4N//DaUHg45GROqBkn6sG/qfsHczLNaVPCKxQEk/1p12PrQ7A+b8Ccp1g7RItFPSj3VmMPQe7wFsy/8RdDQiUseU9AW6XwatusLHf4Cy0qCjEZE6pKQvEBcHF/wGtn8JX7wYdDQiUoeU9MXT/VKvZ61Z/xcO7A06GhGpI0r64jGDYQ/D/m3w8aNBRyMidURJXw7LyIH+N8K8pyF/QdDRiEgdUNKXI130EKS0g2l3QMm3QUcjImEWjo7R1/s9ZeWZWa5f1sLMZprZan+c6pebmY03szVmttjM+tX2/SXMkk6By8dD0Qp4+25wLuiIRCSMwnWkf55zLts5l+O/vg/40DmXBXzovwavE/UsfxgHTAjT+0s4fedCOPd+WPQyzH4i6GhEJIzqqnlnJPC8P/08cEVI+QvOMw9oflRH6hIpzrkXTh8Nsx5W4heJIrXqI9fngPfNzAH/65ybBLRxzhX687cAbfzpdGBTyLL5fllhSBlmNg7vlwAdO3YMQ4hSbXFxcMUEr3nno995l3Fe+KB3lY+INFjhSPpDnHMFZtYamGlmK0NnOuecv0OoMn/HMQkgJydHjcpBiYuHKydC4xT49M/w9Ua49Elokhp0ZCJSQ7Vu3nHOFfjjbcCbwABga0WzjT/e5lcvADqELJ7hl0mkiouH7/0RLngAlk+Dp86CZf/QCV6RBqpWSd/Mks0spWIaGAYsBaYDY/1qY4Fp/vR04Hr/Kp6BwO6QZiCJVGbeI5hv/QiS0+C1sTDpu7ByBpSXBx2diFRDbZt32gBvmtfOmwBMds69a2bzgSlmdjOwAbjGrz8DGAGsAb4Bbqzl+0t9ap8N4z6Bxa/C7MfglTHQogvk3OT1udu0RdARishJmIvwn+k5OTkuNzc36DDkaGUlXnPP/L/CxrleJ+u9roTTr4HO3/WahUQkEGa2IOQS+iOE40SuxKL4ROhztTdsXeYl/yVTvWv7m7XxOl3vMwra99UVPyIRREf6Ej4lxbD6PVg8BVa/D2UHoeV3vOTfZxS0PC3oCEViwomO9JX0pW58uwuWT4clr8H6OYDzOmLvfTX0uBSa6/4LkbqipC/B2l0AS6fC4tdg6xKvrH1f6HEZ9LgcWmUFG59IlFHSl8ixYy2seMsbCvy/a1p3L/n3uAza9tE5AJFaUtKXyLQ7H1b+09sBbPgUXDmkZnrJv9sIyBgA8brWQKS6lPQl8u3ffngHsO5jKC+BpFPhtAug68Xekz+TWwUdpUiDoEs2JfIlt4L+Y72heLeX+Fe9710FtOwNwLwTwV0vhqyLoO0Z3kPhRKRadKQvka28HLYs8ncA70HBQsB59wJkXQRZw6DLeV7nLyICqHlHosm+IljzgbcDWPMRHNgNcQnQcZD/K+Bi72ognQyWGKakL9GprBQ2febtAFbPhG3LvfLmnbwdQJfzoNMgPQpaYo6SvsSGrzd6yX/1+7DuEyj9FjDvMtDMoZA5BDqdDU2aBx2pSJ1S0pfYU1IMBQu8u4HX/ws2fQ5lBwCDdqdDp8GQcaY3nJqh5iCJKkr6IiXF3s1g6+d4Q/58KC325qW0g4ycwzuBdtnQqGmw8YrUgi7ZFElM8pp3Mod4r8tKYMsSyM/1dgD58717BMA7Mdym9+GdQEaO12+Afg1IFNCRvkiFfUXer4GKnUDBQji4z5uXdCq07gWte0CbntC6pzetk8QSgXSkL1IVzdKg2yXeAFBeBttWQP7nULjYm17yGuTuObxMSnt/J9Dj8E4hrRskNgnmM4icRI2Tvpl1AF7A6zLRAZOcc38xsweBW4Eiv+r9zrkZ/jK/BG4GyoA7nXPv1SJ2kboVFw9te3tDBedgTwFsXe5dIloxfDXb6z8AwOK85qDW/i+CNj2hVVfvcdKNkoP5LCK+2hzplwL3OOcW+p2jL5TMZ2AAAAeOSURBVDCzmf68PznnngitbGY9gWuBXkB74AMz6+qcK6tFDCL1y8y72ufUDOg67HB5WSnsXAfblnm/CLYu84YVb+EdE/mS07z7CFI7eePmHb27i5PTvEdRJKd5OwadP5A6UuOk75wrBAr96b1mtgJIP8EiI4FXnHMHgK/MbA0wAJhb0xhEIkZ8AqR19YZeVx4uP/gNFK30dgi71sPXG2DXBu9y0mX/gMqOeRKaHLkTSE7z7i1okuoNSadC4xRv3KSFVy+puZ5FJFUSljZ9M8sE+gKfAYOBn5jZ9UAu3q+BXXg7hHkhi+VznJ2EmY0DxgF07KgelqQBa9QU0vt5w9HKSmHfFthf5D1ldH9RyOC/3rfFu8qo+Gso+eb472Px0LQlNG0BjU/xdgqNm3njRv50o2Z+2SmHpxv5dRqneNP6lRH1ap30zawZ8DrwM+fcHjObAPwO7zft74A/AjdVZ53OuUnAJPCu3qltjCIRKT7hcFNRVZR86z2BtHg3FO/xnjv0zU5vB/GNv5P49ms4sMfrrnL3JjiwDw7s9a9Cqsq/klWyQ2h21I7jeGUpxy4bn1ibLSR1oFZJ38wS8RL+S865NwCcc1tD5j8DvO2/LAA6hCye4ZeJSFUkNvGGlLbVX7a83PulcHCftyM4uNfbGRzY55ftPTzvwF5//r7DZd9s8HYmFWUVJ61PJiEpZEcQ+usjZEdR8bri8yUkVW2sHUqN1ObqHQP+Bqxwzj0ZUt7Ob+8HuBJY6k9PByab2ZN4J3KzgM9r+v4iUg1xcX7CbQYpYVhf6cHDO4vQHcbBo3YkR8zzy/Ztg4PrDu9gSvbXLAaLP3ZnEJ/o3VyX0Nh7ndAY4htDQqOQcSOIS/Tqxif6rxO8cXwj7xfYoelEv26jI+tXvM8RQ7wXU+jrY+oEf96lNkf6g4EfAUvMLM8vux8YY2bZeL8l1wP/AeCcW2ZmU4DleFf+3KErd0QaqIRGkNDCO4dQW+Vl3g6h5FtvKC2uwrjYe6De0eOyEigvhdID3vDtrsPTZQe8nVV5iVevrMT7xVLfaejoHUXF9KEdhj9u1hpunBH2t6/N1TtzgMrO+Bw3SufcI8AjNX1PEYlCcfHelUhJpwbz/uXlITuCg/6OI2S6ory81C+rqFPmlZWXHjVd6u1Iji479PpEy5Ufnm7UrE4+ru7IFZHYFhcHcY29pqAYEHwDk4iI1BslfRGRGKKkLyISQ5T0RURiiJK+iEgMUdIXEYkhSvoiIjFESV9EJIZEfB+5ZlYEbKjh4q2A7WEMp6HT9jhM2+JI2h5Haujbo5NzLq2yGRGf9GvDzHKP1zlwLNL2OEzb4kjaHkeK5u2h5h0RkRiipC8iEkOiPelPCjqACKPtcZi2xZG0PY4Utdsjqtv0RUTkSNF+pC8iIiGU9EVEYkhUJn0zG25mX5rZGjO7L+h46oOZdTCzWWa23MyWmdldfnkLM5tpZqv9capfbmY23t9Gi82sX7CfIPzMLN7MvjCzt/3Xnc3sM/8zv2pmjfzyxv7rNf78zCDjrgtm1tzMpprZSjNbYWaDYvy7cbf/f7LUzF42s6RY+X5EXdI3s3jgKeASoCden709g42qXpQC9zjnegIDgTv8z30f8KFzLgv40H8N3vbJ8odxwIT6D7nO3QWsCHn9B+BPzrnvALuAm/3ym4Fdfvmf/HrR5i/Au8657sAZeNslJr8bZpYO3AnkOOd6A/HAtcTK98M5F1UDMAh4L+T1L4FfBh1XANthGnAR8CXQzi9rB3zpT/8vMCak/qF60TAAGXiJ7Hzgbbz+nLcDCUd/T4D3gEH+dIJfz4L+DGHcFqcCXx39mWL4u5EObAJa+H/vt4GLY+X7EXVH+hz+g1bI98tihv/zsy/wGdDGOVfoz9oCtPGno307/Rn4OVDuv24JfO2cK/Vfh37eQ9vCn7/brx8tOgNFwN/95q6/mlkyMfrdcM4VAE8AG4FCvL/3AmLk+xGNST+mmVkz4HXgZ865PaHznHeoEvXX6JrZpcA259yCoGOJEAlAP2CCc64vsJ/DTTlA7Hw3APxzFyPxdobtgWRgeKBB1aNoTPoFQIeQ1xl+WdQzs0S8hP+Sc+4Nv3irmbXz57cDtvnl0bydBgOXm9l64BW8Jp6/AM3NLMGvE/p5D20Lf/6pwI76DLiO5QP5zrnP/NdT8XYCsfjdALgQ+Mo5V+ScKwHewPvOxMT3IxqT/nwgyz8T3wjvBM30gGOqc2ZmwN+AFc65J0NmTQfG+tNj8dr6K8qv96/UGAjsDvmp36A5537pnMtwzmXi/f0/cs79AJgFXO1XO3pbVGyjq/36UXPU65zbAmwys25+0QXAcmLwu+HbCAw0s6b+/03F9oiN70fQJxXqYgBGAKuAtcB/BR1PPX3mIXg/zxcDef4wAq/t8UNgNfAB0MKvb3hXOa0FluBdyRD456iD7XIu8LY/3QX4HFgDvAY09suT/Ndr/Pldgo67DrZDNpDrfz/+AaTG8ncD+C2wElgKvAg0jpXvhx7DICISQ6KxeUdERI5DSV9EJIYo6YuIxBAlfRGRGKKkLyISQ5T0RURiiJK+iEgM+f87V9EjKF9/5wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGEno4ZBv2ZU",
        "colab_type": "text"
      },
      "source": [
        "We can see a decrease in both the training loss and the validation loss . There is no visible increase in the validation set, which means we can technically continued training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQWgxctBvO4h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import mean_squared_error,mean_absolute_error, explained_variance_score"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IS40RnZ4wbBW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predict(X_test)"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfigbFWFweHa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "761d8652-0099-45c9-f7dd-d6f830ce0fc5"
      },
      "source": [
        "mse = mean_squared_error(y_test, predictions)\n",
        "mse"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "76302.32811851558"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaFxeHzfwjjp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0aaa163c-b847-4e45-816e-6761d330df09"
      },
      "source": [
        "# mean absolute error\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "mae"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "202.64927225626872"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1_d3pUz8FTI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "9b7e1bad-bcd3-45d5-9783-5a02d13b7e0d"
      },
      "source": [
        "rental_df['rent'].describe()"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count     835.000000\n",
              "mean     2125.008383\n",
              "std       363.559143\n",
              "min      1000.000000\n",
              "25%      1937.500000\n",
              "50%      2100.000000\n",
              "75%      2287.500000\n",
              "max      8495.000000\n",
              "Name: rent, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fq-go2Diw1Hw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3625b9dd-ca5b-4d22-cfb2-69b8c2ab0454"
      },
      "source": [
        "mae / rental_df['rent'].mean()"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.09536398719891453"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9O6EjHew4FZ",
        "colab_type": "text"
      },
      "source": [
        "to see if the MAE is at the right level, find the mean of the dependent variable and see what percentage of the mae is compared to the mean, in this case, (mae / 2125 = 8.91%). \n",
        "\n",
        "Use the \"explained_variance_score\" function from the sklearn library to get a better understanding of what's happening. (best possible score is 1.0). It tells us how much variance is explained by the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mv2jn8TZxKMe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "75e9886d-3934-404b-a424-cb91f83752f9"
      },
      "source": [
        "explained_variance_score(y_test, predictions)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.30108721590193543"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozs9zctp0FMh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "7babc445-c66b-479f-927f-bbe2d5dc709d"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xz55pouxywW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "4975a30f-402f-4566-d3e3-91be749173ad"
      },
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "plt.scatter(y_test,predictions)\n",
        "# plot the perfect prediction line\n",
        "plt.plot(y_test,y_test,'r')"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fc7ddfde860>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4wGKnNC0k0I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "42c710ad-1752-4fb4-b892-f10fa82b349e"
      },
      "source": [
        "rental_df.head()"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rent</th>\n",
              "      <th>num_sqft</th>\n",
              "      <th>num_bathrooms</th>\n",
              "      <th>metro_station</th>\n",
              "      <th>train_station</th>\n",
              "      <th>bus_station</th>\n",
              "      <th>bus_stop</th>\n",
              "      <th>shopping_mall</th>\n",
              "      <th>shopping_plaza</th>\n",
              "      <th>grocery_store</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1925.0</td>\n",
              "      <td>750.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5609</td>\n",
              "      <td>4931</td>\n",
              "      <td>3924</td>\n",
              "      <td>2912</td>\n",
              "      <td>17655</td>\n",
              "      <td>8666</td>\n",
              "      <td>3345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1950.0</td>\n",
              "      <td>650.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5610</td>\n",
              "      <td>4944</td>\n",
              "      <td>3921</td>\n",
              "      <td>2916</td>\n",
              "      <td>17644</td>\n",
              "      <td>8651</td>\n",
              "      <td>3335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1950.0</td>\n",
              "      <td>650.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5655</td>\n",
              "      <td>4799</td>\n",
              "      <td>4011</td>\n",
              "      <td>2915</td>\n",
              "      <td>17815</td>\n",
              "      <td>8816</td>\n",
              "      <td>3493</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1850.0</td>\n",
              "      <td>650.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5524</td>\n",
              "      <td>4876</td>\n",
              "      <td>3848</td>\n",
              "      <td>2823</td>\n",
              "      <td>17627</td>\n",
              "      <td>8707</td>\n",
              "      <td>3303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2000.0</td>\n",
              "      <td>650.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4721</td>\n",
              "      <td>4652</td>\n",
              "      <td>3074</td>\n",
              "      <td>2070</td>\n",
              "      <td>17170</td>\n",
              "      <td>8896</td>\n",
              "      <td>2796</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     rent  num_sqft  ...  shopping_plaza  grocery_store\n",
              "0  1925.0     750.0  ...            8666           3345\n",
              "1  1950.0     650.0  ...            8651           3335\n",
              "2  1950.0     650.0  ...            8816           3493\n",
              "3  1850.0     650.0  ...            8707           3303\n",
              "4  2000.0     650.0  ...            8896           2796\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PYCI_j90-5o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "single_property = rental_df.iloc[0,1:]"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtNUkCKD1Kfh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "514a1dda-9ae8-4fcd-b89e-cd3dfa9d0c49"
      },
      "source": [
        "# can't just put the data in the model because it's not scaled and in the right shape\n",
        "single_property.values.reshape(-1, 9)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[7.5000e+02, 1.0000e+00, 5.6090e+03, 4.9310e+03, 3.9240e+03,\n",
              "        2.9120e+03, 1.7655e+04, 8.6660e+03, 3.3450e+03]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lon_buW31FNX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "single_property = scaler.transform(single_property.values.reshape(-1,9))"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IhAUsMf2Bro",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3acda46d-4a79-4a6b-b336-f62987f99768"
      },
      "source": [
        "model.predict(single_property)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2049.1172]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WUIYw512F8Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "outputId": "1892a69c-7236-41d4-8bcb-36912a397a5a"
      },
      "source": [
        "rental_df.head(1)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rent</th>\n",
              "      <th>num_sqft</th>\n",
              "      <th>num_bathrooms</th>\n",
              "      <th>metro_station</th>\n",
              "      <th>train_station</th>\n",
              "      <th>bus_station</th>\n",
              "      <th>bus_stop</th>\n",
              "      <th>shopping_mall</th>\n",
              "      <th>shopping_plaza</th>\n",
              "      <th>grocery_store</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1925.0</td>\n",
              "      <td>750.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5609</td>\n",
              "      <td>4931</td>\n",
              "      <td>3924</td>\n",
              "      <td>2912</td>\n",
              "      <td>17655</td>\n",
              "      <td>8666</td>\n",
              "      <td>3345</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     rent  num_sqft  ...  shopping_plaza  grocery_store\n",
              "0  1925.0     750.0  ...            8666           3345\n",
              "\n",
              "[1 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCrun0on3cQ-",
        "colab_type": "text"
      },
      "source": [
        "the predicted value is off by around $124"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a5alvfW2CU5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vO5kscKb42Kt",
        "colab_type": "text"
      },
      "source": [
        "dropout layers: can be added to layers to \"turn off\" neurons udring training to prevent overfitting. Each dropout layer will 'drop' a user-defined percentage of neuron units in the previous layer every batch\n",
        "\n",
        "EarlyStopping: verbose=1 to get a report back, and patience=25 which waits 25 epochs even after a stopping point is determined."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usehGYDFOQps",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(9, activation='relu'))\n",
        "model.add(Dense(9, activation='relu'))\n",
        "model.add(Dense(9, activation='relu'))\n",
        "model.add(Dense(9, activation='relu'))\n",
        "\n",
        "# one more layer with just one neuron since this is outputting the predicted price\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.compile(optimizer = 'adam', loss = 'mae')\n"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ic6Mdz6tTPZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toLy6DSGS0FD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XnxbPTQ6KDn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2c02b59a-c7a1-491d-e82b-6e2a14574d35"
      },
      "source": [
        "# monitor if the network doesn't import for 25 consecutive epochs than stop training\n",
        "es = EarlyStopping(monitor='val_loss', patience=25, mode='min')\n",
        "\n",
        "# train the model\n",
        "# check with test set as we train using the validation_data parameter\n",
        "model.fit(x=X_train, y=y_train, \n",
        "          validation_data = (X_test, y_test),\n",
        "          # set batch size so we don't pass in the entire training set at once (prevent overfitting)\n",
        "          # focus on smaller batch\n",
        "          batch_size = 128,\n",
        "          callbacks=[es],\n",
        "          epochs = 1800)"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1800\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 238.3545 - val_loss: 227.4117\n",
            "Epoch 2/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 236.9612 - val_loss: 224.2616\n",
            "Epoch 3/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 236.0768 - val_loss: 222.8395\n",
            "Epoch 4/1800\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 235.1073 - val_loss: 222.4955\n",
            "Epoch 5/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 234.2412 - val_loss: 222.3056\n",
            "Epoch 6/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 233.4361 - val_loss: 221.5361\n",
            "Epoch 7/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 232.5942 - val_loss: 221.4185\n",
            "Epoch 8/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 231.8171 - val_loss: 220.6940\n",
            "Epoch 9/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 231.0091 - val_loss: 219.9812\n",
            "Epoch 10/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 230.1866 - val_loss: 218.4505\n",
            "Epoch 11/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 229.4563 - val_loss: 217.8783\n",
            "Epoch 12/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 228.7077 - val_loss: 218.1244\n",
            "Epoch 13/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 228.0197 - val_loss: 217.5717\n",
            "Epoch 14/1800\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 227.4885 - val_loss: 217.8130\n",
            "Epoch 15/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 226.9397 - val_loss: 216.7606\n",
            "Epoch 16/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 226.1828 - val_loss: 215.4731\n",
            "Epoch 17/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 225.8506 - val_loss: 213.9537\n",
            "Epoch 18/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 225.1893 - val_loss: 213.9802\n",
            "Epoch 19/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 224.2333 - val_loss: 214.8416\n",
            "Epoch 20/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 223.9559 - val_loss: 214.2719\n",
            "Epoch 21/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 223.3724 - val_loss: 212.4091\n",
            "Epoch 22/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 222.5687 - val_loss: 211.8977\n",
            "Epoch 23/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 221.9883 - val_loss: 211.8739\n",
            "Epoch 24/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 221.4834 - val_loss: 211.7862\n",
            "Epoch 25/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 220.9896 - val_loss: 210.5352\n",
            "Epoch 26/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 220.4985 - val_loss: 210.0736\n",
            "Epoch 27/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 219.9513 - val_loss: 209.6945\n",
            "Epoch 28/1800\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 219.4498 - val_loss: 209.5426\n",
            "Epoch 29/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 218.9606 - val_loss: 209.5406\n",
            "Epoch 30/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 218.4726 - val_loss: 208.6971\n",
            "Epoch 31/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 217.9447 - val_loss: 207.6996\n",
            "Epoch 32/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 217.8964 - val_loss: 207.1000\n",
            "Epoch 33/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 217.2850 - val_loss: 207.5427\n",
            "Epoch 34/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 216.7643 - val_loss: 207.6695\n",
            "Epoch 35/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 216.4061 - val_loss: 207.8789\n",
            "Epoch 36/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 216.0550 - val_loss: 207.5839\n",
            "Epoch 37/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 215.6112 - val_loss: 206.7764\n",
            "Epoch 38/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 215.0581 - val_loss: 205.9151\n",
            "Epoch 39/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 214.7654 - val_loss: 205.2141\n",
            "Epoch 40/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 214.2988 - val_loss: 205.8080\n",
            "Epoch 41/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 214.0611 - val_loss: 205.4157\n",
            "Epoch 42/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 213.6367 - val_loss: 204.8524\n",
            "Epoch 43/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 213.1389 - val_loss: 205.1855\n",
            "Epoch 44/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 212.9550 - val_loss: 204.5330\n",
            "Epoch 45/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 212.4856 - val_loss: 204.3103\n",
            "Epoch 46/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 212.2080 - val_loss: 204.0045\n",
            "Epoch 47/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 211.7594 - val_loss: 202.7934\n",
            "Epoch 48/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 211.7460 - val_loss: 202.4271\n",
            "Epoch 49/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 211.2762 - val_loss: 202.8384\n",
            "Epoch 50/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 210.9599 - val_loss: 203.4281\n",
            "Epoch 51/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 210.6443 - val_loss: 202.7277\n",
            "Epoch 52/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 210.6776 - val_loss: 201.8331\n",
            "Epoch 53/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 210.1804 - val_loss: 202.7719\n",
            "Epoch 54/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 209.7348 - val_loss: 201.8799\n",
            "Epoch 55/1800\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 209.5720 - val_loss: 200.9047\n",
            "Epoch 56/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 209.5183 - val_loss: 200.6460\n",
            "Epoch 57/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 209.1561 - val_loss: 201.0762\n",
            "Epoch 58/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 208.4854 - val_loss: 200.7179\n",
            "Epoch 59/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 208.3880 - val_loss: 200.2869\n",
            "Epoch 60/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 207.9790 - val_loss: 200.9147\n",
            "Epoch 61/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 207.7333 - val_loss: 201.0426\n",
            "Epoch 62/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 207.5617 - val_loss: 200.0597\n",
            "Epoch 63/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 207.4317 - val_loss: 199.7306\n",
            "Epoch 64/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 207.0203 - val_loss: 200.2755\n",
            "Epoch 65/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 206.8821 - val_loss: 201.1180\n",
            "Epoch 66/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 206.7801 - val_loss: 200.5388\n",
            "Epoch 67/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 206.3125 - val_loss: 199.8605\n",
            "Epoch 68/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 205.9082 - val_loss: 199.2131\n",
            "Epoch 69/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 205.9362 - val_loss: 199.1215\n",
            "Epoch 70/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 205.6363 - val_loss: 199.2922\n",
            "Epoch 71/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 205.5017 - val_loss: 199.7096\n",
            "Epoch 72/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 205.0338 - val_loss: 199.1080\n",
            "Epoch 73/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 204.8776 - val_loss: 198.8759\n",
            "Epoch 74/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 204.6759 - val_loss: 198.9957\n",
            "Epoch 75/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 204.4021 - val_loss: 199.1790\n",
            "Epoch 76/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 204.3470 - val_loss: 199.8103\n",
            "Epoch 77/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 204.4209 - val_loss: 199.1052\n",
            "Epoch 78/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 203.9345 - val_loss: 198.7658\n",
            "Epoch 79/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 203.7131 - val_loss: 198.8062\n",
            "Epoch 80/1800\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 203.4928 - val_loss: 198.9615\n",
            "Epoch 81/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 203.3002 - val_loss: 198.6487\n",
            "Epoch 82/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 203.1388 - val_loss: 198.3544\n",
            "Epoch 83/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 203.0327 - val_loss: 198.2618\n",
            "Epoch 84/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 202.8070 - val_loss: 198.2408\n",
            "Epoch 85/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 202.5894 - val_loss: 198.0602\n",
            "Epoch 86/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 202.3378 - val_loss: 198.2970\n",
            "Epoch 87/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 202.3220 - val_loss: 198.4280\n",
            "Epoch 88/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 202.1042 - val_loss: 198.2853\n",
            "Epoch 89/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 201.8447 - val_loss: 198.1407\n",
            "Epoch 90/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 201.6254 - val_loss: 198.0435\n",
            "Epoch 91/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 201.4431 - val_loss: 197.9927\n",
            "Epoch 92/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 201.2975 - val_loss: 197.8593\n",
            "Epoch 93/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 201.3712 - val_loss: 198.0320\n",
            "Epoch 94/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 201.1624 - val_loss: 197.8291\n",
            "Epoch 95/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 200.8075 - val_loss: 197.9502\n",
            "Epoch 96/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 200.4440 - val_loss: 197.8425\n",
            "Epoch 97/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 200.0927 - val_loss: 197.8831\n",
            "Epoch 98/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 200.0034 - val_loss: 197.9890\n",
            "Epoch 99/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 199.8750 - val_loss: 197.9506\n",
            "Epoch 100/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 199.6649 - val_loss: 197.8528\n",
            "Epoch 101/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 199.4519 - val_loss: 197.8201\n",
            "Epoch 102/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 199.4855 - val_loss: 197.7613\n",
            "Epoch 103/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 199.1800 - val_loss: 197.6534\n",
            "Epoch 104/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 198.9745 - val_loss: 197.5318\n",
            "Epoch 105/1800\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 198.7247 - val_loss: 197.4296\n",
            "Epoch 106/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 198.6887 - val_loss: 197.3889\n",
            "Epoch 107/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 198.4704 - val_loss: 197.3212\n",
            "Epoch 108/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 198.2721 - val_loss: 197.2570\n",
            "Epoch 109/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 198.2082 - val_loss: 197.1916\n",
            "Epoch 110/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 198.0621 - val_loss: 197.2260\n",
            "Epoch 111/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 197.9706 - val_loss: 197.1190\n",
            "Epoch 112/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 197.5991 - val_loss: 197.1211\n",
            "Epoch 113/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 197.8091 - val_loss: 196.9538\n",
            "Epoch 114/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 197.4300 - val_loss: 196.6478\n",
            "Epoch 115/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 197.3735 - val_loss: 196.5556\n",
            "Epoch 116/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 197.2217 - val_loss: 196.3770\n",
            "Epoch 117/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 197.4768 - val_loss: 197.0913\n",
            "Epoch 118/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 197.4971 - val_loss: 196.7105\n",
            "Epoch 119/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 197.0414 - val_loss: 196.4084\n",
            "Epoch 120/1800\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 196.5753 - val_loss: 196.1170\n",
            "Epoch 121/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 196.4520 - val_loss: 196.1659\n",
            "Epoch 122/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 196.1597 - val_loss: 195.8968\n",
            "Epoch 123/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 196.0670 - val_loss: 195.9283\n",
            "Epoch 124/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 195.9907 - val_loss: 195.7552\n",
            "Epoch 125/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 195.7755 - val_loss: 195.5946\n",
            "Epoch 126/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 195.5260 - val_loss: 195.4604\n",
            "Epoch 127/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 195.4135 - val_loss: 195.4095\n",
            "Epoch 128/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 195.5465 - val_loss: 195.4821\n",
            "Epoch 129/1800\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 195.1250 - val_loss: 195.1040\n",
            "Epoch 130/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 194.9666 - val_loss: 194.9632\n",
            "Epoch 131/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 194.9543 - val_loss: 194.9109\n",
            "Epoch 132/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 195.1538 - val_loss: 195.1898\n",
            "Epoch 133/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 194.6584 - val_loss: 195.0016\n",
            "Epoch 134/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 194.9259 - val_loss: 194.7032\n",
            "Epoch 135/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 194.3844 - val_loss: 194.9850\n",
            "Epoch 136/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 194.2544 - val_loss: 194.8185\n",
            "Epoch 137/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 193.9959 - val_loss: 194.2733\n",
            "Epoch 138/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 193.9699 - val_loss: 194.1585\n",
            "Epoch 139/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 193.7456 - val_loss: 194.2211\n",
            "Epoch 140/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 193.8133 - val_loss: 194.5144\n",
            "Epoch 141/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 193.4512 - val_loss: 193.8438\n",
            "Epoch 142/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 193.7255 - val_loss: 193.8862\n",
            "Epoch 143/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 193.6853 - val_loss: 193.5450\n",
            "Epoch 144/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 193.3131 - val_loss: 194.1062\n",
            "Epoch 145/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 193.3940 - val_loss: 193.8676\n",
            "Epoch 146/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 192.9392 - val_loss: 193.2066\n",
            "Epoch 147/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 192.8817 - val_loss: 193.0785\n",
            "Epoch 148/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 192.4643 - val_loss: 193.2352\n",
            "Epoch 149/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 192.6117 - val_loss: 193.7313\n",
            "Epoch 150/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 192.7010 - val_loss: 193.4019\n",
            "Epoch 151/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 192.2472 - val_loss: 192.8129\n",
            "Epoch 152/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 192.2247 - val_loss: 192.7151\n",
            "Epoch 153/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 191.9415 - val_loss: 192.8451\n",
            "Epoch 154/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 191.7666 - val_loss: 192.5863\n",
            "Epoch 155/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 191.6086 - val_loss: 192.4580\n",
            "Epoch 156/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 191.5691 - val_loss: 192.3136\n",
            "Epoch 157/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 191.5114 - val_loss: 193.0071\n",
            "Epoch 158/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 191.8758 - val_loss: 193.5018\n",
            "Epoch 159/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 191.7800 - val_loss: 192.7080\n",
            "Epoch 160/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 190.9999 - val_loss: 192.2158\n",
            "Epoch 161/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 190.9460 - val_loss: 192.0616\n",
            "Epoch 162/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 190.8875 - val_loss: 192.2580\n",
            "Epoch 163/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 190.9148 - val_loss: 192.6527\n",
            "Epoch 164/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 190.7631 - val_loss: 192.1562\n",
            "Epoch 165/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 190.2822 - val_loss: 191.7624\n",
            "Epoch 166/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 190.7329 - val_loss: 191.6550\n",
            "Epoch 167/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 190.2910 - val_loss: 191.7048\n",
            "Epoch 168/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 190.0625 - val_loss: 191.7966\n",
            "Epoch 169/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 189.9931 - val_loss: 191.6788\n",
            "Epoch 170/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 189.8218 - val_loss: 191.2883\n",
            "Epoch 171/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 189.6786 - val_loss: 191.2433\n",
            "Epoch 172/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 189.5165 - val_loss: 190.9391\n",
            "Epoch 173/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 189.4175 - val_loss: 190.8293\n",
            "Epoch 174/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 189.4629 - val_loss: 190.7366\n",
            "Epoch 175/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 189.1299 - val_loss: 191.4043\n",
            "Epoch 176/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 189.4870 - val_loss: 191.0820\n",
            "Epoch 177/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 188.9472 - val_loss: 190.6106\n",
            "Epoch 178/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 189.8066 - val_loss: 190.3562\n",
            "Epoch 179/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 188.8160 - val_loss: 190.6449\n",
            "Epoch 180/1800\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 188.8158 - val_loss: 190.8257\n",
            "Epoch 181/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 188.7153 - val_loss: 190.1349\n",
            "Epoch 182/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 188.6447 - val_loss: 190.0022\n",
            "Epoch 183/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 188.2750 - val_loss: 190.2246\n",
            "Epoch 184/1800\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 188.4446 - val_loss: 190.5971\n",
            "Epoch 185/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 188.1547 - val_loss: 189.8904\n",
            "Epoch 186/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 188.0514 - val_loss: 189.5717\n",
            "Epoch 187/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 188.1687 - val_loss: 189.4413\n",
            "Epoch 188/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 187.8063 - val_loss: 189.8695\n",
            "Epoch 189/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 187.8194 - val_loss: 189.9975\n",
            "Epoch 190/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 187.6626 - val_loss: 189.4395\n",
            "Epoch 191/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 187.4469 - val_loss: 189.4208\n",
            "Epoch 192/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 187.3410 - val_loss: 189.0354\n",
            "Epoch 193/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 187.4376 - val_loss: 188.9220\n",
            "Epoch 194/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 187.0732 - val_loss: 188.9252\n",
            "Epoch 195/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 186.8675 - val_loss: 189.2719\n",
            "Epoch 196/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 187.0257 - val_loss: 188.8979\n",
            "Epoch 197/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 186.7932 - val_loss: 188.4777\n",
            "Epoch 198/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 186.5408 - val_loss: 188.5718\n",
            "Epoch 199/1800\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 186.9150 - val_loss: 188.7603\n",
            "Epoch 200/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 186.3534 - val_loss: 188.5089\n",
            "Epoch 201/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 186.2371 - val_loss: 188.4835\n",
            "Epoch 202/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 186.1337 - val_loss: 188.4339\n",
            "Epoch 203/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 186.2641 - val_loss: 188.2295\n",
            "Epoch 204/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 185.9029 - val_loss: 188.1159\n",
            "Epoch 205/1800\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 185.8554 - val_loss: 188.1932\n",
            "Epoch 206/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 185.7040 - val_loss: 188.0708\n",
            "Epoch 207/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 185.5679 - val_loss: 187.9530\n",
            "Epoch 208/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 185.4020 - val_loss: 187.8262\n",
            "Epoch 209/1800\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 185.3987 - val_loss: 187.7953\n",
            "Epoch 210/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 185.1891 - val_loss: 188.2767\n",
            "Epoch 211/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 185.4858 - val_loss: 188.3512\n",
            "Epoch 212/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 184.9665 - val_loss: 187.6093\n",
            "Epoch 213/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 184.8418 - val_loss: 187.5929\n",
            "Epoch 214/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 185.0155 - val_loss: 187.5642\n",
            "Epoch 215/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 184.7585 - val_loss: 187.4423\n",
            "Epoch 216/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 184.8943 - val_loss: 189.0697\n",
            "Epoch 217/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 185.2699 - val_loss: 187.5652\n",
            "Epoch 218/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 184.2749 - val_loss: 187.0085\n",
            "Epoch 219/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 184.1134 - val_loss: 186.8451\n",
            "Epoch 220/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 183.9998 - val_loss: 186.7807\n",
            "Epoch 221/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 183.9283 - val_loss: 186.5182\n",
            "Epoch 222/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 183.8770 - val_loss: 186.4090\n",
            "Epoch 223/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 183.6723 - val_loss: 186.4191\n",
            "Epoch 224/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 183.6572 - val_loss: 186.4404\n",
            "Epoch 225/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 183.7853 - val_loss: 186.9609\n",
            "Epoch 226/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 183.5292 - val_loss: 186.1264\n",
            "Epoch 227/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 183.4466 - val_loss: 186.0367\n",
            "Epoch 228/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 183.1575 - val_loss: 186.0346\n",
            "Epoch 229/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 183.5629 - val_loss: 186.5838\n",
            "Epoch 230/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 183.1509 - val_loss: 185.9764\n",
            "Epoch 231/1800\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 183.0111 - val_loss: 185.9594\n",
            "Epoch 232/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 182.8020 - val_loss: 186.0548\n",
            "Epoch 233/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 182.8406 - val_loss: 185.9144\n",
            "Epoch 234/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 182.7472 - val_loss: 185.8273\n",
            "Epoch 235/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 182.5600 - val_loss: 185.9381\n",
            "Epoch 236/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 182.5958 - val_loss: 185.7272\n",
            "Epoch 237/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 182.5219 - val_loss: 185.5950\n",
            "Epoch 238/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 182.2823 - val_loss: 185.4280\n",
            "Epoch 239/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 182.3866 - val_loss: 185.2925\n",
            "Epoch 240/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 182.0329 - val_loss: 185.5101\n",
            "Epoch 241/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 182.4974 - val_loss: 185.7442\n",
            "Epoch 242/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 182.2868 - val_loss: 185.0930\n",
            "Epoch 243/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 182.0993 - val_loss: 185.0701\n",
            "Epoch 244/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 181.7130 - val_loss: 185.2077\n",
            "Epoch 245/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 181.7983 - val_loss: 185.0555\n",
            "Epoch 246/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 181.8307 - val_loss: 185.5618\n",
            "Epoch 247/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 181.6840 - val_loss: 184.9753\n",
            "Epoch 248/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 181.4925 - val_loss: 184.9453\n",
            "Epoch 249/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 181.3069 - val_loss: 185.4287\n",
            "Epoch 250/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 181.7562 - val_loss: 186.1145\n",
            "Epoch 251/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 181.5116 - val_loss: 184.8358\n",
            "Epoch 252/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 181.3566 - val_loss: 184.6261\n",
            "Epoch 253/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 181.4531 - val_loss: 184.5168\n",
            "Epoch 254/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 181.1589 - val_loss: 184.6766\n",
            "Epoch 255/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 181.1322 - val_loss: 184.3978\n",
            "Epoch 256/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 181.0312 - val_loss: 184.5382\n",
            "Epoch 257/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 180.9073 - val_loss: 186.1607\n",
            "Epoch 258/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 181.3131 - val_loss: 184.6662\n",
            "Epoch 259/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 180.7761 - val_loss: 184.1616\n",
            "Epoch 260/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 180.8052 - val_loss: 184.3217\n",
            "Epoch 261/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 180.5854 - val_loss: 184.2534\n",
            "Epoch 262/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 180.3405 - val_loss: 184.7231\n",
            "Epoch 263/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 180.4629 - val_loss: 184.5298\n",
            "Epoch 264/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 180.3212 - val_loss: 184.3988\n",
            "Epoch 265/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 180.1796 - val_loss: 184.1811\n",
            "Epoch 266/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 180.0593 - val_loss: 183.8577\n",
            "Epoch 267/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 180.1038 - val_loss: 183.6464\n",
            "Epoch 268/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 180.1346 - val_loss: 183.9561\n",
            "Epoch 269/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 179.8140 - val_loss: 183.5523\n",
            "Epoch 270/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 179.8391 - val_loss: 183.4457\n",
            "Epoch 271/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 179.8570 - val_loss: 183.3117\n",
            "Epoch 272/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 179.8487 - val_loss: 183.1837\n",
            "Epoch 273/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 180.0218 - val_loss: 183.0805\n",
            "Epoch 274/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 179.7453 - val_loss: 183.3566\n",
            "Epoch 275/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 179.4816 - val_loss: 183.6663\n",
            "Epoch 276/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 179.6741 - val_loss: 183.4582\n",
            "Epoch 277/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 179.3887 - val_loss: 183.0818\n",
            "Epoch 278/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 179.5521 - val_loss: 182.9189\n",
            "Epoch 279/1800\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 179.3393 - val_loss: 183.8638\n",
            "Epoch 280/1800\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 179.2166 - val_loss: 183.1895\n",
            "Epoch 281/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 179.1422 - val_loss: 182.7957\n",
            "Epoch 282/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 179.2612 - val_loss: 182.7478\n",
            "Epoch 283/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 179.0742 - val_loss: 183.3222\n",
            "Epoch 284/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 179.0282 - val_loss: 182.9185\n",
            "Epoch 285/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 178.9820 - val_loss: 182.4866\n",
            "Epoch 286/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 179.0095 - val_loss: 182.6520\n",
            "Epoch 287/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 178.7086 - val_loss: 182.9383\n",
            "Epoch 288/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 179.3737 - val_loss: 183.9664\n",
            "Epoch 289/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 179.3580 - val_loss: 182.4451\n",
            "Epoch 290/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 178.5944 - val_loss: 182.7440\n",
            "Epoch 291/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 178.5482 - val_loss: 183.2941\n",
            "Epoch 292/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 179.0394 - val_loss: 183.4451\n",
            "Epoch 293/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 178.5343 - val_loss: 182.5322\n",
            "Epoch 294/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 178.4516 - val_loss: 182.5728\n",
            "Epoch 295/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 178.3596 - val_loss: 182.4102\n",
            "Epoch 296/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 178.2175 - val_loss: 182.8836\n",
            "Epoch 297/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 178.6372 - val_loss: 183.6169\n",
            "Epoch 298/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 178.2761 - val_loss: 182.3005\n",
            "Epoch 299/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 178.9517 - val_loss: 182.1509\n",
            "Epoch 300/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 178.8446 - val_loss: 182.2041\n",
            "Epoch 301/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 178.3906 - val_loss: 182.9406\n",
            "Epoch 302/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 178.1897 - val_loss: 181.9651\n",
            "Epoch 303/1800\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 177.9589 - val_loss: 181.9425\n",
            "Epoch 304/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 177.9252 - val_loss: 182.1854\n",
            "Epoch 305/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 177.9069 - val_loss: 182.8705\n",
            "Epoch 306/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 178.1551 - val_loss: 182.5742\n",
            "Epoch 307/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 177.9295 - val_loss: 181.4632\n",
            "Epoch 308/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 178.0473 - val_loss: 181.4690\n",
            "Epoch 309/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 177.7839 - val_loss: 181.7062\n",
            "Epoch 310/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 177.7522 - val_loss: 181.5992\n",
            "Epoch 311/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 177.6199 - val_loss: 181.7286\n",
            "Epoch 312/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 177.5268 - val_loss: 182.1170\n",
            "Epoch 313/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 177.6699 - val_loss: 181.9352\n",
            "Epoch 314/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 177.3087 - val_loss: 181.2396\n",
            "Epoch 315/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 177.4692 - val_loss: 181.1294\n",
            "Epoch 316/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 177.4680 - val_loss: 181.2264\n",
            "Epoch 317/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 177.4167 - val_loss: 181.8078\n",
            "Epoch 318/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 177.4019 - val_loss: 181.5097\n",
            "Epoch 319/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 177.2102 - val_loss: 181.2804\n",
            "Epoch 320/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 177.1389 - val_loss: 181.0409\n",
            "Epoch 321/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 177.3307 - val_loss: 180.7769\n",
            "Epoch 322/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 177.6093 - val_loss: 180.8073\n",
            "Epoch 323/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 176.8488 - val_loss: 181.7533\n",
            "Epoch 324/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 177.2787 - val_loss: 181.5387\n",
            "Epoch 325/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 176.8190 - val_loss: 180.7280\n",
            "Epoch 326/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 176.9673 - val_loss: 180.6015\n",
            "Epoch 327/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 177.0456 - val_loss: 180.5860\n",
            "Epoch 328/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 176.8253 - val_loss: 180.9356\n",
            "Epoch 329/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 176.6918 - val_loss: 181.0582\n",
            "Epoch 330/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 176.9671 - val_loss: 180.7722\n",
            "Epoch 331/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 176.7082 - val_loss: 181.0879\n",
            "Epoch 332/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 176.6873 - val_loss: 180.6078\n",
            "Epoch 333/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 176.5924 - val_loss: 180.8480\n",
            "Epoch 334/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 176.5831 - val_loss: 181.1889\n",
            "Epoch 335/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 176.5807 - val_loss: 180.6268\n",
            "Epoch 336/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 176.3730 - val_loss: 181.1838\n",
            "Epoch 337/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 176.5693 - val_loss: 181.0376\n",
            "Epoch 338/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 176.5309 - val_loss: 181.1885\n",
            "Epoch 339/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 176.6431 - val_loss: 180.2038\n",
            "Epoch 340/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 176.2516 - val_loss: 180.6200\n",
            "Epoch 341/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 176.3546 - val_loss: 181.5335\n",
            "Epoch 342/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 176.6919 - val_loss: 180.7807\n",
            "Epoch 343/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 176.2398 - val_loss: 180.9409\n",
            "Epoch 344/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 176.1261 - val_loss: 180.3173\n",
            "Epoch 345/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 176.2082 - val_loss: 180.3316\n",
            "Epoch 346/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 176.0929 - val_loss: 180.7009\n",
            "Epoch 347/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 175.9384 - val_loss: 180.1188\n",
            "Epoch 348/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 176.0323 - val_loss: 180.0112\n",
            "Epoch 349/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 175.8659 - val_loss: 179.9052\n",
            "Epoch 350/1800\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 175.7771 - val_loss: 180.4420\n",
            "Epoch 351/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 176.3752 - val_loss: 181.0326\n",
            "Epoch 352/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 175.9829 - val_loss: 179.8284\n",
            "Epoch 353/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 175.7777 - val_loss: 179.5653\n",
            "Epoch 354/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 175.8713 - val_loss: 180.1992\n",
            "Epoch 355/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 175.6220 - val_loss: 179.9552\n",
            "Epoch 356/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 175.4701 - val_loss: 179.5438\n",
            "Epoch 357/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 175.7505 - val_loss: 179.5085\n",
            "Epoch 358/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 175.5894 - val_loss: 180.0696\n",
            "Epoch 359/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 175.5199 - val_loss: 179.6680\n",
            "Epoch 360/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 175.4852 - val_loss: 179.7320\n",
            "Epoch 361/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 175.3606 - val_loss: 179.4879\n",
            "Epoch 362/1800\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 175.3773 - val_loss: 179.6543\n",
            "Epoch 363/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 175.9615 - val_loss: 180.6554\n",
            "Epoch 364/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 175.5808 - val_loss: 179.4331\n",
            "Epoch 365/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 175.2251 - val_loss: 179.9074\n",
            "Epoch 366/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 175.4474 - val_loss: 180.7134\n",
            "Epoch 367/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 175.3782 - val_loss: 179.5188\n",
            "Epoch 368/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 175.1827 - val_loss: 179.6496\n",
            "Epoch 369/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 175.1822 - val_loss: 180.2135\n",
            "Epoch 370/1800\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 175.1983 - val_loss: 179.1486\n",
            "Epoch 371/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 175.8871 - val_loss: 178.9498\n",
            "Epoch 372/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 175.2494 - val_loss: 179.9701\n",
            "Epoch 373/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 175.0941 - val_loss: 179.6487\n",
            "Epoch 374/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 174.9357 - val_loss: 179.0145\n",
            "Epoch 375/1800\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 175.2275 - val_loss: 179.2373\n",
            "Epoch 376/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 175.1790 - val_loss: 180.1350\n",
            "Epoch 377/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 174.8497 - val_loss: 179.1580\n",
            "Epoch 378/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 174.8755 - val_loss: 178.9720\n",
            "Epoch 379/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 174.9788 - val_loss: 179.0487\n",
            "Epoch 380/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 175.1557 - val_loss: 178.6353\n",
            "Epoch 381/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 174.9690 - val_loss: 178.9126\n",
            "Epoch 382/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 174.5626 - val_loss: 179.5671\n",
            "Epoch 383/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 175.0045 - val_loss: 179.8074\n",
            "Epoch 384/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 174.7750 - val_loss: 178.6906\n",
            "Epoch 385/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 175.2834 - val_loss: 178.4659\n",
            "Epoch 386/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 174.6749 - val_loss: 179.3849\n",
            "Epoch 387/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 175.1447 - val_loss: 180.3392\n",
            "Epoch 388/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 174.9376 - val_loss: 179.0578\n",
            "Epoch 389/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 174.6112 - val_loss: 179.3356\n",
            "Epoch 390/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 174.5109 - val_loss: 179.1471\n",
            "Epoch 391/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 174.4007 - val_loss: 178.7710\n",
            "Epoch 392/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 174.5248 - val_loss: 178.7919\n",
            "Epoch 393/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 174.3661 - val_loss: 179.9928\n",
            "Epoch 394/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 174.9193 - val_loss: 179.9660\n",
            "Epoch 395/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 174.1802 - val_loss: 178.4536\n",
            "Epoch 396/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 174.6035 - val_loss: 178.4268\n",
            "Epoch 397/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 174.4447 - val_loss: 179.3091\n",
            "Epoch 398/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 174.3250 - val_loss: 178.5596\n",
            "Epoch 399/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 174.2735 - val_loss: 178.5948\n",
            "Epoch 400/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 174.5405 - val_loss: 178.0419\n",
            "Epoch 401/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 174.3415 - val_loss: 178.5878\n",
            "Epoch 402/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 174.0631 - val_loss: 179.4146\n",
            "Epoch 403/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 174.4158 - val_loss: 179.0382\n",
            "Epoch 404/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 174.1737 - val_loss: 178.0571\n",
            "Epoch 405/1800\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 174.4433 - val_loss: 178.1811\n",
            "Epoch 406/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 174.1826 - val_loss: 178.5676\n",
            "Epoch 407/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 174.3149 - val_loss: 179.5792\n",
            "Epoch 408/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 174.2384 - val_loss: 178.2567\n",
            "Epoch 409/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 174.3601 - val_loss: 178.0208\n",
            "Epoch 410/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 174.2158 - val_loss: 179.2996\n",
            "Epoch 411/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 174.2419 - val_loss: 179.3307\n",
            "Epoch 412/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 174.1147 - val_loss: 178.2444\n",
            "Epoch 413/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 173.9186 - val_loss: 178.9753\n",
            "Epoch 414/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 174.0284 - val_loss: 178.9513\n",
            "Epoch 415/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 174.0606 - val_loss: 178.7289\n",
            "Epoch 416/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 174.3309 - val_loss: 177.9809\n",
            "Epoch 417/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 174.4123 - val_loss: 179.6061\n",
            "Epoch 418/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 174.0496 - val_loss: 178.6562\n",
            "Epoch 419/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 173.8650 - val_loss: 178.7117\n",
            "Epoch 420/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 173.9329 - val_loss: 178.4630\n",
            "Epoch 421/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 174.0855 - val_loss: 179.1793\n",
            "Epoch 422/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 173.8312 - val_loss: 178.3851\n",
            "Epoch 423/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 174.2539 - val_loss: 177.7621\n",
            "Epoch 424/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 174.2122 - val_loss: 178.2538\n",
            "Epoch 425/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 173.7235 - val_loss: 179.2874\n",
            "Epoch 426/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 174.0625 - val_loss: 178.9903\n",
            "Epoch 427/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 173.9660 - val_loss: 177.8715\n",
            "Epoch 428/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 173.7052 - val_loss: 178.3239\n",
            "Epoch 429/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 173.6714 - val_loss: 178.7274\n",
            "Epoch 430/1800\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 173.7576 - val_loss: 178.3584\n",
            "Epoch 431/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 173.5932 - val_loss: 177.9612\n",
            "Epoch 432/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 173.7062 - val_loss: 177.5329\n",
            "Epoch 433/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 174.3992 - val_loss: 177.4378\n",
            "Epoch 434/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 173.5640 - val_loss: 178.8100\n",
            "Epoch 435/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 173.9196 - val_loss: 178.0471\n",
            "Epoch 436/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 173.9261 - val_loss: 178.5450\n",
            "Epoch 437/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 173.7006 - val_loss: 178.2703\n",
            "Epoch 438/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 173.5560 - val_loss: 177.7074\n",
            "Epoch 439/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 173.8989 - val_loss: 177.3639\n",
            "Epoch 440/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 173.7946 - val_loss: 177.5675\n",
            "Epoch 441/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 173.5665 - val_loss: 177.6656\n",
            "Epoch 442/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 173.3540 - val_loss: 178.0877\n",
            "Epoch 443/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 173.5890 - val_loss: 177.5666\n",
            "Epoch 444/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 173.4121 - val_loss: 177.5994\n",
            "Epoch 445/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 173.3723 - val_loss: 177.7111\n",
            "Epoch 446/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 173.4966 - val_loss: 178.0519\n",
            "Epoch 447/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 173.2579 - val_loss: 177.5630\n",
            "Epoch 448/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 173.4497 - val_loss: 177.6767\n",
            "Epoch 449/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 173.5038 - val_loss: 177.3012\n",
            "Epoch 450/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 173.3238 - val_loss: 177.8134\n",
            "Epoch 451/1800\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 173.1928 - val_loss: 178.2496\n",
            "Epoch 452/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 173.3536 - val_loss: 177.9155\n",
            "Epoch 453/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 173.1290 - val_loss: 177.3751\n",
            "Epoch 454/1800\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 173.2811 - val_loss: 177.4902\n",
            "Epoch 455/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 173.2392 - val_loss: 177.7505\n",
            "Epoch 456/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 173.2406 - val_loss: 177.4655\n",
            "Epoch 457/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 173.2949 - val_loss: 177.4170\n",
            "Epoch 458/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 173.2076 - val_loss: 178.1186\n",
            "Epoch 459/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 173.2496 - val_loss: 177.4533\n",
            "Epoch 460/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 173.2160 - val_loss: 177.4791\n",
            "Epoch 461/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 173.0867 - val_loss: 177.6514\n",
            "Epoch 462/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 173.0660 - val_loss: 177.6474\n",
            "Epoch 463/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 173.0342 - val_loss: 177.7530\n",
            "Epoch 464/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 173.0679 - val_loss: 178.0550\n",
            "Epoch 465/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 173.0503 - val_loss: 177.5014\n",
            "Epoch 466/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 173.2906 - val_loss: 177.1380\n",
            "Epoch 467/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 172.9921 - val_loss: 177.7848\n",
            "Epoch 468/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 173.4465 - val_loss: 177.9583\n",
            "Epoch 469/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 172.9701 - val_loss: 177.0046\n",
            "Epoch 470/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 173.2284 - val_loss: 177.0593\n",
            "Epoch 471/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 172.9717 - val_loss: 177.7468\n",
            "Epoch 472/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 172.8837 - val_loss: 177.2709\n",
            "Epoch 473/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 173.0296 - val_loss: 177.0826\n",
            "Epoch 474/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 172.8896 - val_loss: 177.4434\n",
            "Epoch 475/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 172.9792 - val_loss: 177.7213\n",
            "Epoch 476/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 172.9223 - val_loss: 177.4107\n",
            "Epoch 477/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 173.1893 - val_loss: 177.9969\n",
            "Epoch 478/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 173.0296 - val_loss: 177.8357\n",
            "Epoch 479/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 172.8998 - val_loss: 177.0751\n",
            "Epoch 480/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 173.0882 - val_loss: 176.9231\n",
            "Epoch 481/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 173.1625 - val_loss: 178.1711\n",
            "Epoch 482/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 172.9169 - val_loss: 177.3020\n",
            "Epoch 483/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 172.8320 - val_loss: 176.8641\n",
            "Epoch 484/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 172.7227 - val_loss: 177.1966\n",
            "Epoch 485/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 172.7376 - val_loss: 177.4141\n",
            "Epoch 486/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 172.6927 - val_loss: 176.8702\n",
            "Epoch 487/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 172.7535 - val_loss: 177.1308\n",
            "Epoch 488/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 172.6818 - val_loss: 177.6555\n",
            "Epoch 489/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 172.6625 - val_loss: 177.5028\n",
            "Epoch 490/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 172.6195 - val_loss: 176.9662\n",
            "Epoch 491/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 172.6209 - val_loss: 177.2434\n",
            "Epoch 492/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 172.6025 - val_loss: 177.2797\n",
            "Epoch 493/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 172.5586 - val_loss: 177.0372\n",
            "Epoch 494/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 172.6021 - val_loss: 177.1167\n",
            "Epoch 495/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 172.5194 - val_loss: 177.1643\n",
            "Epoch 496/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 172.6055 - val_loss: 177.2811\n",
            "Epoch 497/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 172.4740 - val_loss: 176.9176\n",
            "Epoch 498/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 172.4555 - val_loss: 177.2237\n",
            "Epoch 499/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 172.6418 - val_loss: 177.4106\n",
            "Epoch 500/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 172.2706 - val_loss: 176.6002\n",
            "Epoch 501/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 173.0500 - val_loss: 176.5674\n",
            "Epoch 502/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 172.4506 - val_loss: 177.9291\n",
            "Epoch 503/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 172.8065 - val_loss: 177.1033\n",
            "Epoch 504/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 172.3649 - val_loss: 176.3996\n",
            "Epoch 505/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 172.5451 - val_loss: 176.5493\n",
            "Epoch 506/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 172.4294 - val_loss: 177.2526\n",
            "Epoch 507/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 172.5556 - val_loss: 176.9030\n",
            "Epoch 508/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 172.3866 - val_loss: 177.2669\n",
            "Epoch 509/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 172.5941 - val_loss: 177.8375\n",
            "Epoch 510/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 172.3660 - val_loss: 177.0757\n",
            "Epoch 511/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 172.3436 - val_loss: 176.8149\n",
            "Epoch 512/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 172.2080 - val_loss: 177.3830\n",
            "Epoch 513/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 172.3272 - val_loss: 176.8702\n",
            "Epoch 514/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 172.5627 - val_loss: 176.4091\n",
            "Epoch 515/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 172.4217 - val_loss: 177.7007\n",
            "Epoch 516/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 172.5515 - val_loss: 176.6767\n",
            "Epoch 517/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 172.2832 - val_loss: 176.6459\n",
            "Epoch 518/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 172.4614 - val_loss: 176.4456\n",
            "Epoch 519/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 172.3414 - val_loss: 177.1552\n",
            "Epoch 520/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 172.2218 - val_loss: 176.8183\n",
            "Epoch 521/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 172.0679 - val_loss: 176.2883\n",
            "Epoch 522/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 172.7355 - val_loss: 176.2495\n",
            "Epoch 523/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 172.3947 - val_loss: 176.7341\n",
            "Epoch 524/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 172.2319 - val_loss: 176.5110\n",
            "Epoch 525/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 172.2371 - val_loss: 176.9953\n",
            "Epoch 526/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 172.2209 - val_loss: 176.6034\n",
            "Epoch 527/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 172.0482 - val_loss: 176.2067\n",
            "Epoch 528/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 172.3365 - val_loss: 176.3531\n",
            "Epoch 529/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 172.1989 - val_loss: 176.3754\n",
            "Epoch 530/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 172.1324 - val_loss: 176.4980\n",
            "Epoch 531/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 172.0941 - val_loss: 176.7972\n",
            "Epoch 532/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 172.0658 - val_loss: 176.5705\n",
            "Epoch 533/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 172.0380 - val_loss: 176.5399\n",
            "Epoch 534/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 172.1680 - val_loss: 176.6365\n",
            "Epoch 535/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 172.1069 - val_loss: 176.2352\n",
            "Epoch 536/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 172.1699 - val_loss: 176.8323\n",
            "Epoch 537/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 172.1376 - val_loss: 176.6987\n",
            "Epoch 538/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 171.9958 - val_loss: 176.4643\n",
            "Epoch 539/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 171.9508 - val_loss: 177.4365\n",
            "Epoch 540/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 172.6543 - val_loss: 177.5608\n",
            "Epoch 541/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 172.3647 - val_loss: 176.2773\n",
            "Epoch 542/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 172.0565 - val_loss: 176.4369\n",
            "Epoch 543/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 171.9368 - val_loss: 176.3085\n",
            "Epoch 544/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 171.8929 - val_loss: 176.7995\n",
            "Epoch 545/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 171.8983 - val_loss: 177.5493\n",
            "Epoch 546/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 172.2671 - val_loss: 177.1056\n",
            "Epoch 547/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 172.0820 - val_loss: 176.3638\n",
            "Epoch 548/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 172.1693 - val_loss: 176.4298\n",
            "Epoch 549/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 171.9516 - val_loss: 176.5331\n",
            "Epoch 550/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 172.2140 - val_loss: 176.2759\n",
            "Epoch 551/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 171.9350 - val_loss: 177.5425\n",
            "Epoch 552/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 172.2057 - val_loss: 176.7971\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc7dc907780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oMgXTKIUfZK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBfxRFhM2Ccf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_losses = pd.DataFrame(model.history.history,columns=['training_loss', 'val_loss'])"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WKtKL-rR3k1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "b1ede08d-5365-4e21-cef1-6450fa9d3560"
      },
      "source": [
        "new_losses.plot()"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fc7dc8b1cc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ti84v852CZ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_predictions = model.predict(X_test)"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSSZv5kQR9l6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8cd41445-cf54-49a2-d9cc-5f532175f972"
      },
      "source": [
        "mae = mean_absolute_error(y_test, new_predictions)\n",
        "mae"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "175.93110556231287"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJ6B4IB52CXZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bbb274b6-cf6a-4831-ebc5-39961678acf1"
      },
      "source": [
        "mae / rental_df['rent'].mean()"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.08279078188604891"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EE4KbruzV8FO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "af290689-1240-4fbf-f365-549bb8ade791"
      },
      "source": [
        "explained_variance_score(y_test, new_predictions)"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.29955843375187796"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqYSd9nfWrRI",
        "colab_type": "text"
      },
      "source": [
        "try to see if taking out outliers can help with the model's accuracy. i'll use IQR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_RN7tJ-WCPp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "071cd851-da3b-4cf0-f626-e26cc7358790"
      },
      "source": [
        "Q1 = rental_df['rent'].quantile(0.25)\n",
        "Q3 = rental_df['rent'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "print(IQR)"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "350.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dncg6HtVXB7Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "outliers_df = rental_df.loc[(rental_df['rent'] < (Q1 - 1.5*IQR)) |\\\n",
        "              (rental_df['rent'] > (Q3 + 1.5*IQR))]"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IG5sN2oVXcQN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "61e772ee-f6e5-4534-ecf5-38bc6dbf1ad2"
      },
      "source": [
        "outliers_df.head()"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rent</th>\n",
              "      <th>num_sqft</th>\n",
              "      <th>num_bathrooms</th>\n",
              "      <th>metro_station</th>\n",
              "      <th>train_station</th>\n",
              "      <th>bus_station</th>\n",
              "      <th>bus_stop</th>\n",
              "      <th>shopping_mall</th>\n",
              "      <th>shopping_plaza</th>\n",
              "      <th>grocery_store</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>159</th>\n",
              "      <td>3000.0</td>\n",
              "      <td>800.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>8230</td>\n",
              "      <td>8454</td>\n",
              "      <td>1952</td>\n",
              "      <td>1611</td>\n",
              "      <td>7797</td>\n",
              "      <td>15710</td>\n",
              "      <td>7463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>203</th>\n",
              "      <td>1000.0</td>\n",
              "      <td>300.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6626</td>\n",
              "      <td>6691</td>\n",
              "      <td>4694</td>\n",
              "      <td>3400</td>\n",
              "      <td>5474</td>\n",
              "      <td>12958</td>\n",
              "      <td>4802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>221</th>\n",
              "      <td>3700.0</td>\n",
              "      <td>850.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4041</td>\n",
              "      <td>4096</td>\n",
              "      <td>3636</td>\n",
              "      <td>1975</td>\n",
              "      <td>3022</td>\n",
              "      <td>15439</td>\n",
              "      <td>2269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>259</th>\n",
              "      <td>2950.0</td>\n",
              "      <td>700.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2335</td>\n",
              "      <td>2399</td>\n",
              "      <td>1071</td>\n",
              "      <td>897</td>\n",
              "      <td>1318</td>\n",
              "      <td>16987</td>\n",
              "      <td>645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>262</th>\n",
              "      <td>3200.0</td>\n",
              "      <td>700.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2332</td>\n",
              "      <td>2391</td>\n",
              "      <td>1064</td>\n",
              "      <td>940</td>\n",
              "      <td>1320</td>\n",
              "      <td>17006</td>\n",
              "      <td>669</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       rent  num_sqft  ...  shopping_plaza  grocery_store\n",
              "159  3000.0     800.0  ...           15710           7463\n",
              "203  1000.0     300.0  ...           12958           4802\n",
              "221  3700.0     850.0  ...           15439           2269\n",
              "259  2950.0     700.0  ...           16987            645\n",
              "262  3200.0     700.0  ...           17006            669\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQmCppGrXR7p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filtered_df = rental_df.loc[~rental_df.index.isin(outliers_df.index)]"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GS1FkVzwX3o5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "0f07abff-7dd9-4d30-ad1d-879b536bd423"
      },
      "source": [
        "print(rental_df.shape)\n",
        "print(outliers_df.shape)\n",
        "print(filtered_df.shape)"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(835, 10)\n",
            "(26, 10)\n",
            "(809, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVtZaGVVZSYs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "45340772-8bc7-47a5-ae17-ba80342d33bc"
      },
      "source": [
        "filtered_df.head()"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rent</th>\n",
              "      <th>num_sqft</th>\n",
              "      <th>num_bathrooms</th>\n",
              "      <th>metro_station</th>\n",
              "      <th>train_station</th>\n",
              "      <th>bus_station</th>\n",
              "      <th>bus_stop</th>\n",
              "      <th>shopping_mall</th>\n",
              "      <th>shopping_plaza</th>\n",
              "      <th>grocery_store</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1925.0</td>\n",
              "      <td>750.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5609</td>\n",
              "      <td>4931</td>\n",
              "      <td>3924</td>\n",
              "      <td>2912</td>\n",
              "      <td>17655</td>\n",
              "      <td>8666</td>\n",
              "      <td>3345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1950.0</td>\n",
              "      <td>650.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5610</td>\n",
              "      <td>4944</td>\n",
              "      <td>3921</td>\n",
              "      <td>2916</td>\n",
              "      <td>17644</td>\n",
              "      <td>8651</td>\n",
              "      <td>3335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1950.0</td>\n",
              "      <td>650.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5655</td>\n",
              "      <td>4799</td>\n",
              "      <td>4011</td>\n",
              "      <td>2915</td>\n",
              "      <td>17815</td>\n",
              "      <td>8816</td>\n",
              "      <td>3493</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1850.0</td>\n",
              "      <td>650.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5524</td>\n",
              "      <td>4876</td>\n",
              "      <td>3848</td>\n",
              "      <td>2823</td>\n",
              "      <td>17627</td>\n",
              "      <td>8707</td>\n",
              "      <td>3303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2000.0</td>\n",
              "      <td>650.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4721</td>\n",
              "      <td>4652</td>\n",
              "      <td>3074</td>\n",
              "      <td>2070</td>\n",
              "      <td>17170</td>\n",
              "      <td>8896</td>\n",
              "      <td>2796</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     rent  num_sqft  ...  shopping_plaza  grocery_store\n",
              "0  1925.0     750.0  ...            8666           3345\n",
              "1  1950.0     650.0  ...            8651           3335\n",
              "2  1950.0     650.0  ...            8816           3493\n",
              "3  1850.0     650.0  ...            8707           3303\n",
              "4  2000.0     650.0  ...            8896           2796\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSxZ2buEZXr1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "outputId": "c92ce98e-7be9-4bed-9be7-4f90d8ebac7e"
      },
      "source": [
        "filtered_df.iloc[:,1:].values"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[7.5000e+02, 1.0000e+00, 5.6090e+03, ..., 1.7655e+04, 8.6660e+03,\n",
              "        3.3450e+03],\n",
              "       [6.5000e+02, 1.0000e+00, 5.6100e+03, ..., 1.7644e+04, 8.6510e+03,\n",
              "        3.3350e+03],\n",
              "       [6.5000e+02, 1.0000e+00, 5.6550e+03, ..., 1.7815e+04, 8.8160e+03,\n",
              "        3.4930e+03],\n",
              "       ...,\n",
              "       [6.8800e+02, 1.0000e+00, 2.1750e+03, ..., 3.1470e+03, 2.3750e+04,\n",
              "        4.0200e+02],\n",
              "       [7.4300e+02, 2.0000e+00, 3.6400e+03, ..., 3.8300e+02, 2.5042e+04,\n",
              "        8.1400e+02],\n",
              "       [6.8500e+02, 1.0000e+00, 4.8930e+03, ..., 6.4430e+03, 1.7689e+04,\n",
              "        5.3030e+03]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTuYeuivZMMS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X2 = filtered_df.iloc[:,1:].values\n",
        "y2 = filtered_df['rent'].values"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wZWNPb0ZkL5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size=0.2, random_state=42)"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhKJW2VgZthg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaler = MinMaxScaler()"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jo-5WTvPZwlQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train2 = scaler.fit_transform(X_train2)\n",
        "X_test2 = scaler.transform(X_test2)"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyDAtkUNZMO1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "425e7f51-69f1-4826-8b54-85dfecbe8f3c"
      },
      "source": [
        "X_train2.shape[0] / filtered_df.shape[0]"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.799752781211372"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ELj3SUYYoGj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(9, activation='relu'))\n",
        "model.add(Dense(9, activation='relu'))\n",
        "model.add(Dense(9, activation='relu'))\n",
        "model.add(Dense(9, activation='relu'))\n",
        "\n",
        "# one more layer with just one neuron since this is outputting the predicted price\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.compile(optimizer = 'adam', loss = 'mae')"
      ],
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iG5LOeThZA-K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "01dfdbe6-1e5c-4a7c-f9f5-bb2288908707"
      },
      "source": [
        "# monitor if the network doesn't import for 25 consecutive epochs than stop training\n",
        "es = EarlyStopping(monitor='val_loss', patience=25, mode='min')\n",
        "\n",
        "# train the model\n",
        "# check with test set as we train using the validation_data parameter\n",
        "model.fit(x=X_train2, y=y_train2, \n",
        "          validation_data = (X_test2, y_test2),\n",
        "          # set batch size so we don't pass in the entire training set at once (prevent overfitting)\n",
        "          # focus on smaller batch\n",
        "          batch_size = 128,\n",
        "          callbacks=[es],\n",
        "          epochs = 1800)"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1800\n",
            "6/6 [==============================] - 0s 24ms/step - loss: 2102.5857 - val_loss: 2124.4626\n",
            "Epoch 2/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 2102.5557 - val_loss: 2124.4312\n",
            "Epoch 3/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2102.5234 - val_loss: 2124.3962\n",
            "Epoch 4/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2102.4883 - val_loss: 2124.3577\n",
            "Epoch 5/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2102.4485 - val_loss: 2124.3135\n",
            "Epoch 6/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2102.4031 - val_loss: 2124.2627\n",
            "Epoch 7/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2102.3503 - val_loss: 2124.2041\n",
            "Epoch 8/1800\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 2102.2898 - val_loss: 2124.1355\n",
            "Epoch 9/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 2102.2180 - val_loss: 2124.0544\n",
            "Epoch 10/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2102.1338 - val_loss: 2123.9570\n",
            "Epoch 11/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 2102.0325 - val_loss: 2123.8389\n",
            "Epoch 12/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 2101.9080 - val_loss: 2123.6899\n",
            "Epoch 13/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2101.7478 - val_loss: 2123.4980\n",
            "Epoch 14/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2101.5449 - val_loss: 2123.2666\n",
            "Epoch 15/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2101.3013 - val_loss: 2122.9883\n",
            "Epoch 16/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2101.0063 - val_loss: 2122.6562\n",
            "Epoch 17/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 2100.6550 - val_loss: 2122.2573\n",
            "Epoch 18/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2100.2310 - val_loss: 2121.7769\n",
            "Epoch 19/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2099.7161 - val_loss: 2121.1882\n",
            "Epoch 20/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 2099.0847 - val_loss: 2120.4719\n",
            "Epoch 21/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2098.3193 - val_loss: 2119.6079\n",
            "Epoch 22/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2097.4001 - val_loss: 2118.5750\n",
            "Epoch 23/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2096.3042 - val_loss: 2117.3479\n",
            "Epoch 24/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 2094.9929 - val_loss: 2115.8955\n",
            "Epoch 25/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2093.4565 - val_loss: 2114.1877\n",
            "Epoch 26/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2091.6445 - val_loss: 2112.1875\n",
            "Epoch 27/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 2089.5193 - val_loss: 2109.8542\n",
            "Epoch 28/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2087.0632 - val_loss: 2107.1392\n",
            "Epoch 29/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 2084.1748 - val_loss: 2103.9785\n",
            "Epoch 30/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2080.8218 - val_loss: 2100.3276\n",
            "Epoch 31/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2076.9846 - val_loss: 2096.1060\n",
            "Epoch 32/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2072.4963 - val_loss: 2091.2678\n",
            "Epoch 33/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2067.4050 - val_loss: 2085.7085\n",
            "Epoch 34/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2061.5342 - val_loss: 2079.3118\n",
            "Epoch 35/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2054.7983 - val_loss: 2072.0540\n",
            "Epoch 36/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 2047.1002 - val_loss: 2063.7585\n",
            "Epoch 37/1800\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 2038.3669 - val_loss: 2054.3257\n",
            "Epoch 38/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2028.4662 - val_loss: 2043.6705\n",
            "Epoch 39/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 2017.2678 - val_loss: 2031.6675\n",
            "Epoch 40/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 2004.6385 - val_loss: 2018.2018\n",
            "Epoch 41/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1990.4678 - val_loss: 2003.0615\n",
            "Epoch 42/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1974.6105 - val_loss: 1986.1146\n",
            "Epoch 43/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1956.9308 - val_loss: 1967.3109\n",
            "Epoch 44/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1937.2284 - val_loss: 1946.4677\n",
            "Epoch 45/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 1915.3425 - val_loss: 1923.2305\n",
            "Epoch 46/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 1890.9122 - val_loss: 1897.3036\n",
            "Epoch 47/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1863.8054 - val_loss: 1868.6914\n",
            "Epoch 48/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 1834.0947 - val_loss: 1837.3693\n",
            "Epoch 49/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 1801.4592 - val_loss: 1802.8940\n",
            "Epoch 50/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 1765.4751 - val_loss: 1765.2168\n",
            "Epoch 51/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 1726.1165 - val_loss: 1723.8322\n",
            "Epoch 52/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 1682.8558 - val_loss: 1678.1729\n",
            "Epoch 53/1800\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 1635.2225 - val_loss: 1628.1246\n",
            "Epoch 54/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 1583.3376 - val_loss: 1573.8867\n",
            "Epoch 55/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 1526.7446 - val_loss: 1515.1515\n",
            "Epoch 56/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 1465.7858 - val_loss: 1451.7104\n",
            "Epoch 57/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1400.1956 - val_loss: 1383.1854\n",
            "Epoch 58/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1329.2131 - val_loss: 1309.3654\n",
            "Epoch 59/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1252.2620 - val_loss: 1229.7268\n",
            "Epoch 60/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 1170.0022 - val_loss: 1144.6537\n",
            "Epoch 61/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 1081.2651 - val_loss: 1052.8254\n",
            "Epoch 62/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 986.6151 - val_loss: 954.5201\n",
            "Epoch 63/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 888.0836 - val_loss: 854.7532\n",
            "Epoch 64/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 792.3182 - val_loss: 756.7275\n",
            "Epoch 65/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 703.8093 - val_loss: 668.0699\n",
            "Epoch 66/1800\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 636.9783 - val_loss: 599.2653\n",
            "Epoch 67/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 587.2624 - val_loss: 547.5939\n",
            "Epoch 68/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 548.3860 - val_loss: 509.3053\n",
            "Epoch 69/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 515.6919 - val_loss: 482.2763\n",
            "Epoch 70/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 495.0277 - val_loss: 468.8193\n",
            "Epoch 71/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 482.6920 - val_loss: 460.1971\n",
            "Epoch 72/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 474.7231 - val_loss: 454.2769\n",
            "Epoch 73/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 469.5773 - val_loss: 449.4385\n",
            "Epoch 74/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 465.3074 - val_loss: 445.1895\n",
            "Epoch 75/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 461.1325 - val_loss: 440.6209\n",
            "Epoch 76/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 456.7120 - val_loss: 436.0784\n",
            "Epoch 77/1800\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 452.3421 - val_loss: 431.5103\n",
            "Epoch 78/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 447.5538 - val_loss: 426.8570\n",
            "Epoch 79/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 442.7262 - val_loss: 422.3970\n",
            "Epoch 80/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 438.5417 - val_loss: 418.4822\n",
            "Epoch 81/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 434.3560 - val_loss: 414.3748\n",
            "Epoch 82/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 430.2872 - val_loss: 409.9903\n",
            "Epoch 83/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 426.5664 - val_loss: 405.7911\n",
            "Epoch 84/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 422.3248 - val_loss: 401.6546\n",
            "Epoch 85/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 418.3869 - val_loss: 397.6919\n",
            "Epoch 86/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 414.5777 - val_loss: 393.8071\n",
            "Epoch 87/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 410.8553 - val_loss: 390.0471\n",
            "Epoch 88/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 407.0428 - val_loss: 386.0674\n",
            "Epoch 89/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 403.2365 - val_loss: 382.2645\n",
            "Epoch 90/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 399.6172 - val_loss: 378.6491\n",
            "Epoch 91/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 396.0090 - val_loss: 375.3313\n",
            "Epoch 92/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 392.5193 - val_loss: 372.2744\n",
            "Epoch 93/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 389.0022 - val_loss: 370.0224\n",
            "Epoch 94/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 385.9567 - val_loss: 367.4484\n",
            "Epoch 95/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 382.7399 - val_loss: 363.7487\n",
            "Epoch 96/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 379.2104 - val_loss: 360.6915\n",
            "Epoch 97/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 375.9332 - val_loss: 358.0467\n",
            "Epoch 98/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 372.7245 - val_loss: 354.9158\n",
            "Epoch 99/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 369.5576 - val_loss: 351.8072\n",
            "Epoch 100/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 366.1724 - val_loss: 347.7740\n",
            "Epoch 101/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 362.7629 - val_loss: 343.7512\n",
            "Epoch 102/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 359.7477 - val_loss: 339.9529\n",
            "Epoch 103/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 356.7614 - val_loss: 336.9147\n",
            "Epoch 104/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 353.6729 - val_loss: 333.8992\n",
            "Epoch 105/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 350.7086 - val_loss: 331.1315\n",
            "Epoch 106/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 347.8124 - val_loss: 328.9861\n",
            "Epoch 107/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 344.7747 - val_loss: 325.9590\n",
            "Epoch 108/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 341.9264 - val_loss: 323.0284\n",
            "Epoch 109/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 338.9822 - val_loss: 320.5676\n",
            "Epoch 110/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 336.2043 - val_loss: 317.8362\n",
            "Epoch 111/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 333.6518 - val_loss: 314.9438\n",
            "Epoch 112/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 331.2438 - val_loss: 311.5540\n",
            "Epoch 113/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 329.2663 - val_loss: 308.5020\n",
            "Epoch 114/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 326.6271 - val_loss: 306.1672\n",
            "Epoch 115/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 323.9042 - val_loss: 303.8098\n",
            "Epoch 116/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 321.2103 - val_loss: 302.4148\n",
            "Epoch 117/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 318.7373 - val_loss: 300.7096\n",
            "Epoch 118/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 316.6577 - val_loss: 296.9286\n",
            "Epoch 119/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 315.2176 - val_loss: 293.2960\n",
            "Epoch 120/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 313.4456 - val_loss: 291.1143\n",
            "Epoch 121/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 310.7576 - val_loss: 289.7598\n",
            "Epoch 122/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 308.3862 - val_loss: 288.9586\n",
            "Epoch 123/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 305.9492 - val_loss: 288.6949\n",
            "Epoch 124/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 303.8651 - val_loss: 286.4099\n",
            "Epoch 125/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 302.1611 - val_loss: 282.7802\n",
            "Epoch 126/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 300.5031 - val_loss: 281.1062\n",
            "Epoch 127/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 298.3416 - val_loss: 278.8874\n",
            "Epoch 128/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 296.0924 - val_loss: 278.8487\n",
            "Epoch 129/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 294.2608 - val_loss: 280.1385\n",
            "Epoch 130/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 292.6759 - val_loss: 279.0574\n",
            "Epoch 131/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 290.7516 - val_loss: 275.8373\n",
            "Epoch 132/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 288.8318 - val_loss: 274.5653\n",
            "Epoch 133/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 287.1730 - val_loss: 272.9883\n",
            "Epoch 134/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 285.5088 - val_loss: 272.1989\n",
            "Epoch 135/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 284.1215 - val_loss: 271.9637\n",
            "Epoch 136/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 282.2758 - val_loss: 266.8407\n",
            "Epoch 137/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 280.1854 - val_loss: 262.3224\n",
            "Epoch 138/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 278.2718 - val_loss: 260.4297\n",
            "Epoch 139/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 276.7318 - val_loss: 258.9364\n",
            "Epoch 140/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 275.1039 - val_loss: 258.9649\n",
            "Epoch 141/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 273.4428 - val_loss: 260.8833\n",
            "Epoch 142/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 272.5984 - val_loss: 260.1707\n",
            "Epoch 143/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 270.9935 - val_loss: 256.1276\n",
            "Epoch 144/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 269.1782 - val_loss: 251.2422\n",
            "Epoch 145/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 269.3502 - val_loss: 248.3443\n",
            "Epoch 146/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 269.5599 - val_loss: 247.3296\n",
            "Epoch 147/1800\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 267.3798 - val_loss: 248.1509\n",
            "Epoch 148/1800\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 264.3907 - val_loss: 248.8041\n",
            "Epoch 149/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 263.0508 - val_loss: 248.5130\n",
            "Epoch 150/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 261.9413 - val_loss: 246.3004\n",
            "Epoch 151/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 260.8811 - val_loss: 244.8728\n",
            "Epoch 152/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 259.7351 - val_loss: 244.8039\n",
            "Epoch 153/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 258.1655 - val_loss: 246.2674\n",
            "Epoch 154/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 256.9831 - val_loss: 248.1335\n",
            "Epoch 155/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 256.0744 - val_loss: 248.4916\n",
            "Epoch 156/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 255.5411 - val_loss: 248.9351\n",
            "Epoch 157/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 253.9247 - val_loss: 244.6853\n",
            "Epoch 158/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 252.2301 - val_loss: 240.3720\n",
            "Epoch 159/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 251.1133 - val_loss: 237.8342\n",
            "Epoch 160/1800\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 250.2111 - val_loss: 234.9215\n",
            "Epoch 161/1800\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 249.5225 - val_loss: 233.7617\n",
            "Epoch 162/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 248.7533 - val_loss: 233.7685\n",
            "Epoch 163/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 247.4655 - val_loss: 235.7067\n",
            "Epoch 164/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 246.3181 - val_loss: 237.0842\n",
            "Epoch 165/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 245.4118 - val_loss: 238.4191\n",
            "Epoch 166/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 244.7590 - val_loss: 237.2677\n",
            "Epoch 167/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 243.7771 - val_loss: 233.9829\n",
            "Epoch 168/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 242.6489 - val_loss: 232.2930\n",
            "Epoch 169/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 241.8289 - val_loss: 233.8447\n",
            "Epoch 170/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 240.9610 - val_loss: 233.5193\n",
            "Epoch 171/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 240.0936 - val_loss: 230.7899\n",
            "Epoch 172/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 239.3232 - val_loss: 228.5696\n",
            "Epoch 173/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 238.5856 - val_loss: 227.1564\n",
            "Epoch 174/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 238.0276 - val_loss: 225.3037\n",
            "Epoch 175/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 237.5155 - val_loss: 224.3517\n",
            "Epoch 176/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 236.8959 - val_loss: 223.5769\n",
            "Epoch 177/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 236.3103 - val_loss: 223.2880\n",
            "Epoch 178/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 235.5832 - val_loss: 225.9155\n",
            "Epoch 179/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 234.3768 - val_loss: 225.2793\n",
            "Epoch 180/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 233.8766 - val_loss: 223.8649\n",
            "Epoch 181/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 233.1908 - val_loss: 223.3149\n",
            "Epoch 182/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 232.7377 - val_loss: 224.5524\n",
            "Epoch 183/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 231.8293 - val_loss: 224.8792\n",
            "Epoch 184/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 231.1763 - val_loss: 226.4831\n",
            "Epoch 185/1800\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 230.5509 - val_loss: 225.5818\n",
            "Epoch 186/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 230.4251 - val_loss: 227.9219\n",
            "Epoch 187/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 229.5935 - val_loss: 224.7942\n",
            "Epoch 188/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 228.6600 - val_loss: 224.5937\n",
            "Epoch 189/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 228.0439 - val_loss: 226.6976\n",
            "Epoch 190/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 227.8013 - val_loss: 227.2674\n",
            "Epoch 191/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 227.4649 - val_loss: 226.2248\n",
            "Epoch 192/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 226.5448 - val_loss: 222.7526\n",
            "Epoch 193/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 225.8380 - val_loss: 223.1799\n",
            "Epoch 194/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 225.3624 - val_loss: 225.5130\n",
            "Epoch 195/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 225.2159 - val_loss: 224.1595\n",
            "Epoch 196/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 224.3530 - val_loss: 219.9440\n",
            "Epoch 197/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 223.5540 - val_loss: 219.2467\n",
            "Epoch 198/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 223.0333 - val_loss: 218.0619\n",
            "Epoch 199/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 222.6896 - val_loss: 219.2420\n",
            "Epoch 200/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 222.2794 - val_loss: 219.9675\n",
            "Epoch 201/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 221.5309 - val_loss: 215.7414\n",
            "Epoch 202/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 221.1601 - val_loss: 211.5633\n",
            "Epoch 203/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 220.7511 - val_loss: 211.4221\n",
            "Epoch 204/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 220.1402 - val_loss: 212.4456\n",
            "Epoch 205/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 219.5540 - val_loss: 213.9158\n",
            "Epoch 206/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 218.9758 - val_loss: 216.4955\n",
            "Epoch 207/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 218.8541 - val_loss: 218.1411\n",
            "Epoch 208/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 218.5236 - val_loss: 217.8095\n",
            "Epoch 209/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 218.1081 - val_loss: 217.2230\n",
            "Epoch 210/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 217.6890 - val_loss: 216.5407\n",
            "Epoch 211/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 216.8754 - val_loss: 212.0416\n",
            "Epoch 212/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 216.4380 - val_loss: 206.9228\n",
            "Epoch 213/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 216.3100 - val_loss: 203.5504\n",
            "Epoch 214/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 216.5864 - val_loss: 203.7079\n",
            "Epoch 215/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 215.8453 - val_loss: 205.2340\n",
            "Epoch 216/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 214.7831 - val_loss: 207.1448\n",
            "Epoch 217/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 214.1511 - val_loss: 208.6021\n",
            "Epoch 218/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 213.8777 - val_loss: 211.5103\n",
            "Epoch 219/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 213.6758 - val_loss: 212.1739\n",
            "Epoch 220/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 213.2923 - val_loss: 211.3001\n",
            "Epoch 221/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 212.7311 - val_loss: 209.7855\n",
            "Epoch 222/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 212.2194 - val_loss: 209.3871\n",
            "Epoch 223/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 211.7296 - val_loss: 207.1074\n",
            "Epoch 224/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 211.4531 - val_loss: 202.8894\n",
            "Epoch 225/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 211.1213 - val_loss: 199.4304\n",
            "Epoch 226/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 211.5243 - val_loss: 198.1901\n",
            "Epoch 227/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 211.3191 - val_loss: 198.0847\n",
            "Epoch 228/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 210.1828 - val_loss: 199.8287\n",
            "Epoch 229/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 209.3537 - val_loss: 199.2928\n",
            "Epoch 230/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 209.2046 - val_loss: 197.1080\n",
            "Epoch 231/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 209.0484 - val_loss: 198.5534\n",
            "Epoch 232/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 208.1594 - val_loss: 198.2533\n",
            "Epoch 233/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 207.5413 - val_loss: 199.5953\n",
            "Epoch 234/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 207.1251 - val_loss: 198.1759\n",
            "Epoch 235/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 206.9738 - val_loss: 197.0990\n",
            "Epoch 236/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 206.6488 - val_loss: 199.6358\n",
            "Epoch 237/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 205.9905 - val_loss: 199.4158\n",
            "Epoch 238/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 205.6802 - val_loss: 201.0219\n",
            "Epoch 239/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 205.2834 - val_loss: 201.1517\n",
            "Epoch 240/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 204.9081 - val_loss: 200.8737\n",
            "Epoch 241/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 204.4887 - val_loss: 199.5787\n",
            "Epoch 242/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 204.0529 - val_loss: 198.0033\n",
            "Epoch 243/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 203.5055 - val_loss: 197.8751\n",
            "Epoch 244/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 203.0948 - val_loss: 197.6739\n",
            "Epoch 245/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 202.7279 - val_loss: 196.3749\n",
            "Epoch 246/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 202.2416 - val_loss: 194.9191\n",
            "Epoch 247/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 201.7445 - val_loss: 192.9846\n",
            "Epoch 248/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 201.4847 - val_loss: 191.3856\n",
            "Epoch 249/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 201.9388 - val_loss: 188.8699\n",
            "Epoch 250/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 203.1367 - val_loss: 187.4818\n",
            "Epoch 251/1800\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 201.6236 - val_loss: 190.8347\n",
            "Epoch 252/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 199.8248 - val_loss: 194.6354\n",
            "Epoch 253/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 199.7986 - val_loss: 193.8543\n",
            "Epoch 254/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 199.3798 - val_loss: 193.6152\n",
            "Epoch 255/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 199.0705 - val_loss: 193.5505\n",
            "Epoch 256/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 198.8260 - val_loss: 192.9068\n",
            "Epoch 257/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 198.3754 - val_loss: 190.6320\n",
            "Epoch 258/1800\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 197.9830 - val_loss: 190.8484\n",
            "Epoch 259/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 197.6947 - val_loss: 190.4777\n",
            "Epoch 260/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 197.3241 - val_loss: 189.6292\n",
            "Epoch 261/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 197.2135 - val_loss: 190.7453\n",
            "Epoch 262/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 196.7695 - val_loss: 190.1689\n",
            "Epoch 263/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 196.4733 - val_loss: 191.1484\n",
            "Epoch 264/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 196.3248 - val_loss: 191.4614\n",
            "Epoch 265/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 196.1775 - val_loss: 190.4506\n",
            "Epoch 266/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 195.7292 - val_loss: 187.4001\n",
            "Epoch 267/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 195.4099 - val_loss: 187.7264\n",
            "Epoch 268/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 195.0967 - val_loss: 187.7045\n",
            "Epoch 269/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 194.8728 - val_loss: 187.1668\n",
            "Epoch 270/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 194.6628 - val_loss: 187.2599\n",
            "Epoch 271/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 194.3014 - val_loss: 188.3305\n",
            "Epoch 272/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 194.1397 - val_loss: 187.0421\n",
            "Epoch 273/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 193.9569 - val_loss: 185.9575\n",
            "Epoch 274/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 193.7002 - val_loss: 185.7437\n",
            "Epoch 275/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 193.5034 - val_loss: 184.2680\n",
            "Epoch 276/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 193.6896 - val_loss: 182.6775\n",
            "Epoch 277/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 193.5445 - val_loss: 183.5375\n",
            "Epoch 278/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 192.6460 - val_loss: 185.2239\n",
            "Epoch 279/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 192.2057 - val_loss: 184.9313\n",
            "Epoch 280/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 191.9549 - val_loss: 184.9796\n",
            "Epoch 281/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 191.8420 - val_loss: 186.7805\n",
            "Epoch 282/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 191.7159 - val_loss: 188.8957\n",
            "Epoch 283/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 192.3945 - val_loss: 190.7433\n",
            "Epoch 284/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 192.1447 - val_loss: 187.1360\n",
            "Epoch 285/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 190.9241 - val_loss: 184.6141\n",
            "Epoch 286/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 190.6854 - val_loss: 184.5047\n",
            "Epoch 287/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 190.4811 - val_loss: 184.7681\n",
            "Epoch 288/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 190.2756 - val_loss: 183.7393\n",
            "Epoch 289/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 190.1770 - val_loss: 180.0533\n",
            "Epoch 290/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 191.1320 - val_loss: 178.9985\n",
            "Epoch 291/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 190.6396 - val_loss: 180.9055\n",
            "Epoch 292/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 189.3380 - val_loss: 183.9092\n",
            "Epoch 293/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 189.4818 - val_loss: 187.4478\n",
            "Epoch 294/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 190.3481 - val_loss: 188.1202\n",
            "Epoch 295/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 189.6890 - val_loss: 183.9324\n",
            "Epoch 296/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 188.3810 - val_loss: 181.8217\n",
            "Epoch 297/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 188.1221 - val_loss: 180.7600\n",
            "Epoch 298/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 188.1485 - val_loss: 178.4744\n",
            "Epoch 299/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 188.5468 - val_loss: 177.5316\n",
            "Epoch 300/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 188.6216 - val_loss: 178.2904\n",
            "Epoch 301/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 187.4221 - val_loss: 181.5541\n",
            "Epoch 302/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 187.0874 - val_loss: 180.7747\n",
            "Epoch 303/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 186.8375 - val_loss: 180.2529\n",
            "Epoch 304/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 186.7287 - val_loss: 180.7044\n",
            "Epoch 305/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 186.5293 - val_loss: 178.7467\n",
            "Epoch 306/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 186.3764 - val_loss: 178.1877\n",
            "Epoch 307/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 186.2096 - val_loss: 178.7956\n",
            "Epoch 308/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 185.9762 - val_loss: 179.7775\n",
            "Epoch 309/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 185.8335 - val_loss: 178.7420\n",
            "Epoch 310/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 185.5381 - val_loss: 177.4063\n",
            "Epoch 311/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 185.6201 - val_loss: 177.6564\n",
            "Epoch 312/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 185.3479 - val_loss: 177.2762\n",
            "Epoch 313/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 185.3431 - val_loss: 176.3491\n",
            "Epoch 314/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 185.6657 - val_loss: 175.2280\n",
            "Epoch 315/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 185.8465 - val_loss: 175.0599\n",
            "Epoch 316/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 185.4627 - val_loss: 175.5192\n",
            "Epoch 317/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 184.9022 - val_loss: 175.7168\n",
            "Epoch 318/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 184.4297 - val_loss: 176.7006\n",
            "Epoch 319/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 184.0565 - val_loss: 177.7881\n",
            "Epoch 320/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 183.8675 - val_loss: 177.2421\n",
            "Epoch 321/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 183.7299 - val_loss: 177.2561\n",
            "Epoch 322/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 183.5503 - val_loss: 176.3371\n",
            "Epoch 323/1800\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 183.6234 - val_loss: 174.9219\n",
            "Epoch 324/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 183.6808 - val_loss: 174.6021\n",
            "Epoch 325/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 183.6361 - val_loss: 174.3574\n",
            "Epoch 326/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 183.3942 - val_loss: 174.8269\n",
            "Epoch 327/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 182.9064 - val_loss: 175.4761\n",
            "Epoch 328/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 182.6063 - val_loss: 176.3720\n",
            "Epoch 329/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 182.4723 - val_loss: 176.2909\n",
            "Epoch 330/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 182.3386 - val_loss: 176.0759\n",
            "Epoch 331/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 182.1450 - val_loss: 175.5536\n",
            "Epoch 332/1800\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 182.0957 - val_loss: 176.3437\n",
            "Epoch 333/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 181.9712 - val_loss: 176.1339\n",
            "Epoch 334/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 181.7609 - val_loss: 177.0381\n",
            "Epoch 335/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 181.9867 - val_loss: 177.9610\n",
            "Epoch 336/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 181.8895 - val_loss: 177.5679\n",
            "Epoch 337/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 181.5568 - val_loss: 175.9651\n",
            "Epoch 338/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 181.3217 - val_loss: 173.7826\n",
            "Epoch 339/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 181.3452 - val_loss: 172.7144\n",
            "Epoch 340/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 181.3258 - val_loss: 172.3039\n",
            "Epoch 341/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 181.5609 - val_loss: 171.7517\n",
            "Epoch 342/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 181.3718 - val_loss: 172.0487\n",
            "Epoch 343/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 181.1797 - val_loss: 171.9653\n",
            "Epoch 344/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 180.7395 - val_loss: 173.2151\n",
            "Epoch 345/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 180.2662 - val_loss: 172.7153\n",
            "Epoch 346/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 180.2714 - val_loss: 172.3395\n",
            "Epoch 347/1800\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 180.1463 - val_loss: 172.6284\n",
            "Epoch 348/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 179.8865 - val_loss: 173.4221\n",
            "Epoch 349/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 179.6449 - val_loss: 173.8215\n",
            "Epoch 350/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 179.7553 - val_loss: 175.2432\n",
            "Epoch 351/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 179.8496 - val_loss: 176.0918\n",
            "Epoch 352/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 180.1564 - val_loss: 177.2333\n",
            "Epoch 353/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 180.3295 - val_loss: 176.9666\n",
            "Epoch 354/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 179.9685 - val_loss: 175.4716\n",
            "Epoch 355/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 179.3698 - val_loss: 175.1120\n",
            "Epoch 356/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 179.0228 - val_loss: 173.2404\n",
            "Epoch 357/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 178.6441 - val_loss: 171.4623\n",
            "Epoch 358/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 178.7109 - val_loss: 170.6997\n",
            "Epoch 359/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 178.6150 - val_loss: 171.7526\n",
            "Epoch 360/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 178.3756 - val_loss: 174.6156\n",
            "Epoch 361/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 178.5521 - val_loss: 175.0372\n",
            "Epoch 362/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 178.5556 - val_loss: 174.9973\n",
            "Epoch 363/1800\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 178.5308 - val_loss: 174.3205\n",
            "Epoch 364/1800\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 178.1468 - val_loss: 170.8361\n",
            "Epoch 365/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 177.9920 - val_loss: 169.6198\n",
            "Epoch 366/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 178.5151 - val_loss: 169.5639\n",
            "Epoch 367/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 178.0004 - val_loss: 169.9943\n",
            "Epoch 368/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 177.5259 - val_loss: 170.2453\n",
            "Epoch 369/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 177.2440 - val_loss: 170.7217\n",
            "Epoch 370/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 176.9991 - val_loss: 171.4636\n",
            "Epoch 371/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 176.9554 - val_loss: 171.6828\n",
            "Epoch 372/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 176.8505 - val_loss: 172.0286\n",
            "Epoch 373/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 176.8803 - val_loss: 173.0800\n",
            "Epoch 374/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 176.8842 - val_loss: 173.0248\n",
            "Epoch 375/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 176.6941 - val_loss: 172.2555\n",
            "Epoch 376/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 176.6279 - val_loss: 170.9282\n",
            "Epoch 377/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 176.3874 - val_loss: 170.2982\n",
            "Epoch 378/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 176.4094 - val_loss: 170.6319\n",
            "Epoch 379/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 176.1437 - val_loss: 170.0017\n",
            "Epoch 380/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 176.5738 - val_loss: 169.6319\n",
            "Epoch 381/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 176.7941 - val_loss: 169.9267\n",
            "Epoch 382/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 176.1396 - val_loss: 170.9535\n",
            "Epoch 383/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 175.6507 - val_loss: 171.6033\n",
            "Epoch 384/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 175.7417 - val_loss: 172.3322\n",
            "Epoch 385/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 175.7350 - val_loss: 172.7303\n",
            "Epoch 386/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 175.8147 - val_loss: 173.1287\n",
            "Epoch 387/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 175.6816 - val_loss: 171.5397\n",
            "Epoch 388/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 175.3352 - val_loss: 170.0800\n",
            "Epoch 389/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 175.1262 - val_loss: 170.2013\n",
            "Epoch 390/1800\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 174.8762 - val_loss: 171.8824\n",
            "Epoch 391/1800\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 175.2482 - val_loss: 173.1649\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc7dc7ca7b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIXrUXxqaMFR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions2 = model.predict(X_test2)"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFv4ElQ2as6I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2124922b-3527-49b2-dc10-ab8e7cfd63c7"
      },
      "source": [
        "mae = mean_absolute_error(y_test2, predictions2)\n",
        "mae"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "173.16488609784915"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-smbOY6atUP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b4ea8051-e607-46a3-97ac-77b5d971100d"
      },
      "source": [
        "explained_variance_score(y_test2, predictions2)"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.13536526773384006"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4waS9Mh-fJmw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "losses2 = pd.DataFrame(model.history.history,columns=['training_loss', 'val_loss'])"
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qnZfVwffMgC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "01ea5a64-4cd9-4ac9-c49a-2d906d87a5a2"
      },
      "source": [
        "losses2.plot()"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-181-58bb6874717b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlosses2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/plotting/_core.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    845\u001b[0m                     \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mplot_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m     \u001b[0m__call__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/plotting/_matplotlib/__init__.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(data, kind, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ax\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"left_ax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mplot_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPLOT_CLASSES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mplot_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0mplot_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mplot_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/plotting/_matplotlib/core.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args_adjust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_plot_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_subplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/plotting/_matplotlib/core.py\u001b[0m in \u001b[0;36m_compute_plot_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0;31m# no non-numeric frames or series allowed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_empty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"no numeric data to plot\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0;31m# GH25587: cast ExtensionArray of pandas (IntegerArray, etc.) to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: no numeric data to plot"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fljjUzwDatYN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "660fe458-2738-4ab8-d751-a543f46b3547"
      },
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "plt.scatter(y_test2,predictions2)\n"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7fc7dc738fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 179
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9thedunjbIoa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}